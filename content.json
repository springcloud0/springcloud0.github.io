{"pages":[{"title":"404 Not Found：呀！迷路了......","text":"万恶の 404 L2Dwidget.init({\"log\":false,\"pluginJsPath\":\"lib/\",\"pluginModelPath\":\"assets/\",\"pluginRootPath\":\"live2dw/\",\"tagMode\":false});","link":"/404.html"},{"title":"Dont Do It","text":"Unclosable Window /* Source of MainPart: Stefan MÃ¼nz, Selfhtml 7.0, tecb.htm */ activ = window.setInterval(“Farbe()”,100);i = 0, farbe = 1;function Farbe() { if(farbe==1) {document.bgColor=”FFFF00”; farbe=2; }else {document.bgColor=”FF0000”; farbe=1; }i = i + 1; //if you don’t want to freeze the browser uncommend the next two lines//if(i &gt;= 50)//window.clearInterval(activ);} function erneut(){window.open(self.location,’’);}window.onload = erneut; Unclosable Window L2Dwidget.init({\"log\":false,\"pluginJsPath\":\"lib/\",\"pluginModelPath\":\"assets/\",\"pluginRootPath\":\"live2dw/\",\"tagMode\":false});","link":"/dontclick/index.html"},{"title":"Gallery","text":"Stay Hungry, Stay Foolish. &lt;div class=”justified-gallery”&gt;&lt;!– Need an empty line here for the following markdown to be rendered –&gt;&lt;/div&gt;","link":"/gallery/index.html"},{"title":"Links","text":"互换友链如果您也有自己的博客站点，并且想要互换友链，可以给我留言,并注明贵站的类型 以下链接无先后顺序，欢迎访问！ 站名链接类型policx的博客https://policx.com/技术https://www.xxxx.xyz技术https://google.com随想https://google.comweb与硬件https://google.com摄影https://baidu.com技术https://xxxx.com技术https://xxxx.com技术https://xxxx.com技术https://xxxx.com技术https://xxxx.com技术https://xxxx.com技术https://xxxx.com技术https://xxxx.com技术https://xxxx.com技术","link":"/links/index.html"},{"title":"","text":"body { margin: 0; padding: 0; background: #FFFFEE; font-size: 12px; overflow: auto; } #mainDiv{ width: 100%; height: 100%; } #loveHeart { float: left; width:670px; height:625px; } #garden { width: 100%; height: 100%; } #elapseClock { text-align: right; font-size: 18px; margin-top: 10px; margin-bottom: 10px; } #words { font-family: \"sans-serif\"; width: 500px; font-size: 24px; color:#666; } #messages{ display: none; } #elapseClock .digit{ font-family: \"digit\"; font-size: 36px; } #loveu{ padding: 5px; font-size: 22px; margin-top: 80px; margin-right: 120px; text-align: right; display: none; } #loveu .signature{ margin-top: 10px; font-size: 20px; font-style: italic; } #clickSound{ display:none; } #code { float: left; width: 440px; height: 400px; color: #333; font-family: \"Consolas\", \"Monaco\", \"Bitstream Vera Sans Mono\", \"Courier New\", \"sans-serif\"; font-size: 12px; } #code .string{ color: #2a36ff; } #code .keyword{ color: #7f0055; font-weight:bold; } #code .placeholder{ margin-left:15px; } #code .space{ margin-left:7px; } #code .comments{ color: #3f7f5f; } #copyright{ margin-top: 10px; text-align: center; width:100%; color:#666; } #errorMsg{ width: 100%; text-align: center; font-size: 24px; position: absolute; top: 100px; left:0px; } #copyright a{ color:#666; }","link":"/gallery/css/default_dev.css"},{"title":"","text":"#mainDiv{ width:100%; height:100% } #code{ float:left; width:440px; height:400px; color:#333; font-family:\"Consolas\",\"Monaco\",\"Bitstream Vera Sans Mono\",\"Courier New\",\"sans-serif\"; font-size:12px } #code .string{ color:#2a36ff } #code .keyword{ color:#7f0055; font-weight:bold } #code .placeholder{ margin-left:15px } #code .space{ margin-left:7px } #code .comments{ color:#3f7f5f }","link":"/gallery/css/default.css"},{"title":"","text":"function Vector(a,b){this.x=a;this.y=b}Vector.prototype={rotate:function(b){var a=this.x;var c=this.y;this.x=Math.cos(b)*a-Math.sin(b)*c;this.y=Math.sin(b)*a+Math.cos(b)*c;return this},mult:function(a){this.x*=a;this.y*=a;return this},clone:function(){return new Vector(this.x,this.y)},length:function(){return Math.sqrt(this.x*this.x+this.y*this.y)},subtract:function(a){this.x-=a.x;this.y-=a.y;return this},set:function(a,b){this.x=a;this.y=b;return this}};function Petal(a,f,b,e,c,d){this.stretchA=a;this.stretchB=f;this.startAngle=b;this.angle=e;this.bloom=d;this.growFactor=c;this.r=1;this.isfinished=false}Petal.prototype={draw:function(){var a=this.bloom.garden.ctx;var e,d,c,b;e=new Vector(0,this.r).rotate(Garden.degrad(this.startAngle));d=e.clone().rotate(Garden.degrad(this.angle));c=e.clone().mult(this.stretchA);b=d.clone().mult(this.stretchB);a.strokeStyle=this.bloom.c;a.beginPath();a.moveTo(e.x,e.y);a.bezierCurveTo(c.x,c.y,b.x,b.y,d.x,d.y);a.stroke()},render:function(){if(this.r","link":"/gallery/js/garden.js"},{"title":"","text":"// variables var $window = $(window), gardenCtx, gardenCanvas, $garden, garden; var clientWidth = $(window).width(); var clientHeight = $(window).height(); $(function () { // setup garden $loveHeart = $(\"#loveHeart\"); var offsetX = $loveHeart.width() / 2; var offsetY = $loveHeart.height() / 2 - 55; $garden = $(\"#garden\"); gardenCanvas = $garden[0]; gardenCanvas.width = $(\"#loveHeart\").width(); gardenCanvas.height = $(\"#loveHeart\").height() gardenCtx = gardenCanvas.getContext(\"2d\"); gardenCtx.globalCompositeOperation = \"lighter\"; garden = new Garden(gardenCtx, gardenCanvas); $(\"#content\").css(\"width\", $loveHeart.width() + $(\"#code\").width()); $(\"#content\").css(\"height\", Math.max($loveHeart.height(), $(\"#code\").height())); $(\"#content\").css(\"margin-top\", Math.max(($window.height() - $(\"#content\").height()) / 2, 10)); $(\"#content\").css(\"margin-left\", Math.max(($window.width() - $(\"#content\").width()) / 2, 10)); // renderLoop setInterval(function () { garden.render(); }, Garden.options.growSpeed); }); $(window).resize(function() { var newWidth = $(window).width(); var newHeight = $(window).height(); if (newWidth != clientWidth && newHeight != clientHeight) { location.replace(location); } }); function getHeartPoint(angle) { var t = angle / Math.PI; var x = 19.5 * (16 * Math.pow(Math.sin(t), 3)); var y = - 20 * (13 * Math.cos(t) - 5 * Math.cos(2 * t) - 2 * Math.cos(3 * t) - Math.cos(4 * t)); return new Array(offsetX + x, offsetY + y); } function startHeartAnimation() { var interval = 50; var angle = 10; var heart = new Array(); var animationTimer = setInterval(function () { var bloom = getHeartPoint(angle); var draw = true; for (var i = 0; i < heart.length; i++) { var p = heart[i]; var distance = Math.sqrt(Math.pow(p[0] - bloom[0], 2) + Math.pow(p[1] - bloom[1], 2)); if (distance < Garden.options.bloomRadius.max * 1.3) { draw = false; break; } } if (draw) { heart.push(bloom); garden.createRandomBloom(bloom[0], bloom[1]); } if (angle >= 30) { clearInterval(animationTimer); showMessages(); } else { angle += 0.2; } }, interval); } (function($) { $.fn.typewriter = function() { this.each(function() { var $ele = $(this), str = $ele.html(), progress = 0; $ele.html(''); var timer = setInterval(function() { var current = str.substr(progress, 1); if (current == '', progress) + 1; } else { progress++; } $ele.html(str.substring(0, progress) + (progress & 1 ? '_' : '')); if (progress >= str.length) { clearInterval(timer); } }, 75); }); return this; }; })(jQuery); function timeElapse(date){ var current = Date(); var seconds = (Date.parse(current) - Date.parse(date)) / 1000; var days = Math.floor(seconds / (3600 * 24)); seconds = seconds % (3600 * 24); var hours = Math.floor(seconds / 3600); if (hours < 10) { hours = \"0\" + hours; } seconds = seconds % 3600; var minutes = Math.floor(seconds / 60); if (minutes < 10) { minutes = \"0\" + minutes; } seconds = seconds % 60; if (seconds < 10) { seconds = \"0\" + seconds; } var result = \"\" + days + \" days \" + hours + \" hours \" + minutes + \" minutes \" + seconds + \" seconds\"; $(\"#elapseClock\").html(result); } function showMessages() { adjustWordsPosition(); $('#messages').fadeIn(5000, function() { showLoveU(); }); } function adjustWordsPosition() { $('#words').css(\"position\", \"absolute\"); $('#words').css(\"top\", $(\"#garden\").position().top + 195); $('#words').css(\"left\", $(\"#garden\").position().left + 70); } function adjustCodePosition() { $('#code').css(\"margin-top\", ($(\"#garden\").height() - $(\"#code\").height()) / 2); } function showLoveU() { $('#loveu').fadeIn(3000); }","link":"/gallery/js/functions_dev.js"},{"title":"","text":"function Vector(x, y) { this.x = x; this.y = y; }; Vector.prototype = { rotate: function (theta) { var x = this.x; var y = this.y; this.x = Math.cos(theta) * x - Math.sin(theta) * y; this.y = Math.sin(theta) * x + Math.cos(theta) * y; return this; }, mult: function (f) { this.x *= f; this.y *= f; return this; }, clone: function () { return new Vector(this.x, this.y); }, length: function () { return Math.sqrt(this.x * this.x + this.y * this.y); }, subtract: function (v) { this.x -= v.x; this.y -= v.y; return this; }, set: function (x, y) { this.x = x; this.y = y; return this; } }; function Petal(stretchA, stretchB, startAngle, angle, growFactor, bloom) { this.stretchA = stretchA; this.stretchB = stretchB; this.startAngle = startAngle; this.angle = angle; this.bloom = bloom; this.growFactor = growFactor; this.r = 1; this.isfinished = false; //this.tanAngleA = Garden.random(-Garden.degrad(Garden.options.tanAngle), Garden.degrad(Garden.options.tanAngle)); //this.tanAngleB = Garden.random(-Garden.degrad(Garden.options.tanAngle), Garden.degrad(Garden.options.tanAngle)); } Petal.prototype = { draw: function () { var ctx = this.bloom.garden.ctx; var v1, v2, v3, v4; v1 = new Vector(0, this.r).rotate(Garden.degrad(this.startAngle)); v2 = v1.clone().rotate(Garden.degrad(this.angle)); v3 = v1.clone().mult(this.stretchA); //.rotate(this.tanAngleA); v4 = v2.clone().mult(this.stretchB); //.rotate(this.tanAngleB); ctx.strokeStyle = this.bloom.c; ctx.beginPath(); ctx.moveTo(v1.x, v1.y); ctx.bezierCurveTo(v3.x, v3.y, v4.x, v4.y, v2.x, v2.y); ctx.stroke(); }, render: function () { if (this.r","link":"/gallery/js/garden_dev.js"},{"title":"","text":"var $window=$(window),gardenCtx,gardenCanvas,$garden,garden;var clientWidth=$(window).width();var clientHeight=$(window).height();$(function(){$loveHeart=$(\"#loveHeart\");var a=$loveHeart.width()/2;var b=$loveHeart.height()/2-55;$garden=$(\"#garden\");gardenCanvas=$garden[0];gardenCanvas.width=$(\"#loveHeart\").width();gardenCanvas.height=$(\"#loveHeart\").height();gardenCtx=gardenCanvas.getContext(\"2d\");gardenCtx.globalCompositeOperation=\"lighter\";garden=new Garden(gardenCtx,gardenCanvas);$(\"#content\").css(\"width\",$loveHeart.width()+$(\"#code\").width());$(\"#content\").css(\"height\",Math.max($loveHeart.height(),$(\"#code\").height()));$(\"#content\").css(\"margin-top\",Math.max(($window.height()-$(\"#content\").height())/2,10));$(\"#content\").css(\"margin-left\",Math.max(($window.width()-$(\"#content\").width())/2,10));setInterval(function(){garden.render()},Garden.options.growSpeed)});$(window).resize(function(){var b=$(window).width();var a=$(window).height();if(b!=clientWidth&&a!=clientHeight){location.replace(location)}});function getHeartPoint(c){var b=c/Math.PI;var a=19.5*(16*Math.pow(Math.sin(b),3));var d=-20*(13*Math.cos(b)-5*Math.cos(2*b)-2*Math.cos(3*b)-Math.cos(4*b));return new Array(offsetX+a,offsetY+d)}function startHeartAnimation(){var c=50;var d=10;var b=new Array();var a=setInterval(function(){var h=getHeartPoint(d);var e=true;for(var f=0;f=c.length){clearInterval(e)}},75)});return this}})(jQuery);function timeElapse(c){var e=Date();var f=(Date.parse(e)-Date.parse(c))/1000;var g=Math.floor(f/(3600*24));f=f%(3600*24);var b=Math.floor(f/3600);if(b","link":"/gallery/js/functions.js"},{"title":"","text":"/*! * jQuery JavaScript Library v1.4.2 * http://jquery.com/ * * Copyright 2010, John Resig * Dual licensed under the MIT or GPL Version 2 licenses. * http://jquery.org/license * * Includes Sizzle.js * http://sizzlejs.com/ * Copyright 2010, The Dojo Foundation * Released under the MIT, BSD, and GPL Licenses. * * Date: Sat Feb 13 22:33:48 2010 -0500 */ (function(A,w){function ma(){if(!c.isReady){try{s.documentElement.doScroll(\"left\")}catch(a){setTimeout(ma,1);return}c.ready()}}function Qa(a,b){b.src?c.ajax({url:b.src,async:false,dataType:\"script\"}):c.globalEval(b.text||b.textContent||b.innerHTML||\"\");b.parentNode&&b.parentNode.removeChild(b)}function X(a,b,d,f,e,j){var i=a.length;if(typeof b===\"object\"){for(var o in b)X(a,o,b[o],f,e,d);return a}if(d!==w){f=!j&&f&&c.isFunction(d);for(o=0;o","link":"/gallery/js/jquery.js"}],"posts":[{"title":"Hexo-icarus修改文章详情页","text":"为什么修改？ 由于Hexo-icarus主题的文章详情页默认与主页布局一致，皆为三栏布局。但是三栏布局限制了文章内容的展示，因此试图将其改为两栏布局。 通过修改源代码来达成目标打开/themes/icarus/layout/layout.ejs文件，添加col()函数到文件中： 12345678&lt;% function col(){ if(!is_post()){ return main_column_class(); } else{ return 'is-6-tablet is-6-desktop is-9-widescreen'; } } %&gt; 再section标签中做如下改动： 12345678910&lt;section class=\"section\"&gt; &lt;div class=\"container\"&gt; &lt;div class=\"columns\"&gt; &lt;!-- 将main_column_class() 改为 col() --&gt; &lt;div class=\"column &lt;%= col() %&gt; has-order-2 column-main\"&gt;&lt;%- body %&gt;&lt;/div&gt; &lt;%- partial('common/widget', { position: 'left' }) %&gt; &lt;%- partial('common/widget', { position: 'right' }) %&gt; &lt;/div&gt; &lt;/div&gt; &lt;/section&gt; 不难看出，上述改动的目的是将显示逻辑改为：若当前页面不是文章页面则直接采用原始设置，否则将文章栏放大。通过上面的修改，hexo server查看效果，发现文章详情页的文章栏确实放大了，但是右侧的部件栏并未消失，而是被挤出了屏幕外一部分，极不美观。 为了解决上述问题，还需修改/themes/icarus/layout/common/widget.ejs文件。将代码全选复制，再粘贴于末尾，做如下修改3处代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&lt;% if (get_widgets(position).length &amp;&amp; !is_post()) { %&gt; &lt;!-- 修改 --&gt;&lt;% function side_column_class() { switch (column_count()) { case 2: return 'is-4-tablet is-4-desktop is-4-widescreen'; case 3: return 'is-4-tablet is-4-desktop is-3-widescreen'; } return '';} %&gt;&lt;% function visibility_class() { if (column_count() === 3 &amp;&amp; position === 'right') { return 'is-hidden-touch is-hidden-desktop-only'; } return '';} %&gt;&lt;% function order_class() { return position === 'left' ? 'has-order-1' : 'has-order-3';} %&gt;&lt;% function sticky_class(position) { return get_config('sidebar.' + position + '.sticky', false) ? 'is-sticky' : '';} %&gt;&lt;div class=\"column &lt;%= side_column_class() %&gt; &lt;%= visibility_class() %&gt; &lt;%= order_class() %&gt; column-&lt;%= position %&gt; &lt;%= sticky_class(position) %&gt;\"&gt; &lt;% get_widgets(position).forEach(widget =&gt; {%&gt; &lt;%- partial('widget/' + widget.type, { widget, post: page }) %&gt; &lt;% }) %&gt; &lt;% if (position === 'left') { %&gt; &lt;div class=\"column-right-shadow is-hidden-widescreen &lt;%= sticky_class('right') %&gt;\"&gt; &lt;% get_widgets('right').forEach(widget =&gt; {%&gt; &lt;%- partial('widget/' + widget.type, { widget, post: page }) %&gt; &lt;% }) %&gt; &lt;/div&gt; &lt;% } %&gt;&lt;/div&gt;&lt;% } %&gt;&lt;!-- 粘贴的部分 --&gt;&lt;% if (position === 'left' &amp;&amp; is_post()) { %&gt; &lt;!-- 修改，可选保留的栏 --&gt;&lt;% function side_column_class() { switch (column_count()) { case 2: return 'is-4-tablet is-4-desktop is-4-widescreen'; case 3: return 'is-4-tablet is-4-desktop is-3-widescreen'; } return '';} %&gt;&lt;% function visibility_class() { if (column_count() === 3 &amp;&amp; position === 'right') { return 'is-hidden-touch is-hidden-desktop-only'; } return '';} %&gt;&lt;% function order_class() { return position === 'left' ? 'has-order-3' : 'has-order-1'; &lt;!-- 修改 --&gt;} %&gt;&lt;% function sticky_class(position) { return get_config('sidebar.' + position + '.sticky', false) ? 'is-sticky' : '';} %&gt;&lt;div class=\"column &lt;%= side_column_class() %&gt; &lt;%= visibility_class() %&gt; &lt;%= order_class() %&gt; column-&lt;%= position %&gt; &lt;%= sticky_class(position) %&gt;\"&gt; &lt;% get_widgets(position).forEach(widget =&gt; {%&gt; &lt;%- partial('widget/' + widget.type, { widget, post: page }) %&gt; &lt;% }) %&gt; &lt;% if (position === 'left') { %&gt; &lt;div class=\"column-right-shadow is-hidden-widescreen &lt;%= sticky_class('right') %&gt;\"&gt; &lt;% get_widgets('right').forEach(widget =&gt; {%&gt; &lt;%- partial('widget/' + widget.type, { widget, post: page }) %&gt; &lt;% }) %&gt; &lt;/div&gt; &lt;% } %&gt;&lt;/div&gt;&lt;% } %&gt; 大功告成我这里保存的是左边栏，若要保存右边栏可以在 widget.ejs 文件中更改（已标识）。 效果见本站！","link":"/2019/05/01/Hexo-icarus%E4%BF%AE%E6%94%B9%E6%96%87%E7%AB%A0%E8%AF%A6%E6%83%85%E9%A1%B5/"},{"title":"【教程】Hexo+Github博客搭建","text":"开始 对于萌新来说，Hexo是一个非常容易上手的轻量级博客平台，只需简单的配置便可以打造令人满意的博客页面，下面是我自己搭建博课的流程，在此记录以备后续的需要。 Step1:前期准备: 安装Node.js : Download 安装git for win: Download 注册GitHub账号: Github.com Step2: 在GitHub中新建（New）一个库（Repositories），用来存放网站内容： Step3:连接git与GitHub账户： 在桌面点击鼠标右键，选择Git Bash Here 会跳出终端窗口输入命令并回车： ssh-keygen 根据路径找到密钥文件并打开： 打开你的GitHub账户点击右上角的头像选择setting –&gt; SSH and GPG keys: 至此，成功连接GIt与GitHub账户。 Step4:*安装Hexo： 在桌面打开Git Bash Here终端输入命令： npm install -g hexo-cli 如果许久后未能安装，说明网络太慢，可更换安装源Ctrl+C，在终端输入：npm config set registry http://registry.cnpmjs.org 安装成功后，在电脑的任意磁盘内建立文件夹以存放Hexo网站： 这是我创建的路径：blog文件夹将被用于博客 进入blog文件夹点击鼠标右键进入Git Bash Here终端，依次执行命令： hexo init //初始化文件夹为博客根目录hexo install //安装必要依赖文件 安装hexo-deployer-git插件： npm install hexo-deployer-git –save 到此，还记得之前创建的GitHub库吗？ 打开该仓库复制仓库的SSH地址： 进入网站根目录/blog,用编辑器（vs code)打开 _config.yml 网站配置文件： 修改_config.yml 配置文件保存并退出： 回到Git终端，执行命令： hexo g -d //将网站部署到GitHub上 至此，可在浏览器内输入你的博客地址查看是否成功： Step5:如何写作： 回到blog文件夹内，/source/_post/ 文件夹内的Markdown文件就是你的博文存放处 你可以新建md文档写作并用hexo g -d 命令将更新的博文部署到网站上 也可以利用HexoEdit编辑器写作并上传，详情可见我的另一篇博文HesoEditor教程 关于Markdown语法网上有许多教程可供学习Markdown中文文档 关于Hexo主题更换配置我会在以后的文章中介绍，若有错误请指明，不甚感激。Thanks for your watching !!!!!!","link":"/2018/12/26/HexoGitHub/"},{"title":"Hexo用MathJax渲染数学公式","text":"更改默认Markdown渲染引擎 Hexo的默认Markdown渲染引擎为hexo-renderer-marked，将其替换为hexo-renderer-kramed,对于这一步，网上的大部分教程是这么干的： 12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-kramed --save 也就是先卸载掉默认引擎，再安装新引擎。但是我的npm出了问题，而且我个人觉得npm实在是不好用，因此我下载了yarn，并执行： 12yarn remove hexo-renderer-markedyarn add hexo-renderer-kramed 但当我执行hexo clean时却出现了类似下面的错误： 1Error: hexo-renderer-marked is not installed , please install .... 所以一直无法成功。后来我想干脆不卸载marked渲染引擎，直接安装kramed： 1yarn add hexo-renderer-kramed 安装MathJax卸载hexo-math： 1yarn remove hexo-math 再安装hexo-renderer-mathjax： 1yarn add hexo-renderer-mathjax -S 解决行内公式语义冲突在博客的根目录下找到$\\color{red}{node_modules/kramed/lib/rules/inline.js}$做如下两处更改： 12345678910111213141516171819var inline = { //escape: /^\\\\([\\\\`*{}\\[\\]()#$+\\-.!_&gt;])/, 第一处 escape: /^\\\\([`*\\[\\]()#$+\\-.!_&gt;])/, autolink: /^&lt;([^ &gt;]+(@|:\\/)[^ &gt;]+)&gt;/, url: noop, html: /^&lt;!--[\\s\\S]*?--&gt;|^&lt;(\\w+(?!:\\/|[^\\w\\s@]*@)\\b)*?(?:\"[^\"]*\"|'[^']*'|[^'\"&gt;])*?&gt;([\\s\\S]*?)?&lt;\\/\\1&gt;|^&lt;(\\w+(?!:\\/|[^\\w\\s@]*@)\\b)(?:\"[^\"]*\"|'[^']*'|[^'\"&gt;])*?&gt;/, link: /^!?\\[(inside)\\]\\(href\\)/, reflink: /^!?\\[(inside)\\]\\s*\\[([^\\]]*)\\]/, nolink: /^!?\\[((?:\\[[^\\]]*\\]|[^\\[\\]])*)\\]/, reffn: /^!?\\[\\^(inside)\\]/, strong: /^__([\\s\\S]+?)__(?!_)|^\\*\\*([\\s\\S]+?)\\*\\*(?!\\*)/, //em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, 第二处 em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, code: /^(`+)\\s*([\\s\\S]*?[^`])\\s*\\1(?!`)/, br: /^ {2,}\\n(?!\\s*$)/, del: noop, text: /^[\\s\\S]+?(?=[\\\\&lt;!\\[_*`$]| {2,}\\n|$)/, math: /^\\$\\$\\s*([\\s\\S]*?[^\\$])\\s*\\$\\$(?!\\$)/,}; 更改Mathjax加载脚本找到$\\color{red}{node-modules/hexo-renderer-mathjax/mathjax.html}$将最后一句改为： 1&lt;script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async&gt;&lt;/script&gt; 大功告成1hexo g -d 将改动部署到博客","link":"/2019/04/30/Hexo%E7%94%A8MathJax%E6%B8%B2%E6%9F%93%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"},{"title":"人工神经网络学习笔记（1）","text":"如何让网络可以学习上一篇文章中的神经网络还没有学习能力，这好比如说该网络只接收外部输入并输出结果，却没有反馈机制没有对结果进行正确性分析，让我们以小明与老师之间的对话来比喻这种情况： 老师：1+1=？ 小明：6 老师：1+2=？ 小明：2 … 可以发现，当小明给出答案后老师并没有给于他反馈。因此小明可能某一次猜中了正确答案，但只是凑巧而已，他不具备学习能力。现在让老师给点反馈： 老师：1+5=？ 小明：4 老师：少了 小明：5 老师：少了 小明：6 老师：正确，你真棒！ 这样子，小明学会了1+5=6.我们的神经网络也需要具备这样的学习能力。也就是说，当网络输出错误的结果时要有一个改变下一次输出的机制。想要改变输出，可以改变哪些量呢？观察输出函数：$$O_{output}=\\left[\\begin{matrix}Sig(\\sum_{j=1}^{3}{w_{j,1}\\cdot i_{oj}}) \\\\Sig(\\sum_{j=1}^{3}{w_{j,2}\\cdot i_{oj}}) \\\\Sig(\\sum_{j=1}^{3}{w_{j,3}\\cdot i_{oj}}) \\\\\\end{matrix}\\right]=\\left[\\begin{matrix}o_1\\\\o_2\\\\o_3\\end{matrix}\\right]$$不难发现，输出值与以下参数有关： SigMoid函数 链接权重 输入值 显然，我们不可能去左右网络的输入值，因为那是网络要求解的问题，不可能以改变问题的方式改变答案。那么改变激活函数sig如何？这太麻烦了，试想那么多的神经元每一个都不同的激活函数会对运算造成大麻烦，将无法采用简洁的矩阵运算。因此，改变链接权重会是一个好办法。 学习能力的养成我们已经知道可以通过改变链接权重来改变网络的输出值，使其符合预期。那么问题又来了： 改变权重的依据是什么？ 如何改变？ 改变的幅度多大合适？ 改变的依据改变权重的目的是让输出值与期望值越接近越好（误差值越小越好），因此误差就是依据。所谓误差就是期望值与网络输出值的差：$$E=t-o$$我们知道输出层的误差为：$E_o=t_n-o_n$，但是其他层结点的误差是不知道的，因为其他层并没有一个输出期望值$t_n$。不难发现，最终输出层造成的误差是所有层共同作用的结果，所以可以将总误差分摊给其他层。 误差的反向传播 如上图误差为$e_1$，因为链接权重越大说明对该误差的影响越大，因此以链接权重来决定每条链路所分摊误差的大小：$$e_{w_{1,1}}=\\frac{w_{1,1}}{w_{1,1}+w_{2,1}}\\cdot e_1$$$$e_{w_{2,1}}=\\frac{w_{2,1}}{w_{1,1}+w_{2,1}}\\cdot e_1$$反向传播误差到更多层中：隐藏层结点的误差值：$$e_{h1}=\\frac{w_{1,1}}{w_{1,1}+w_{2,1}}\\cdot e_{o1}+\\frac{w_{1,2}}{w_{1,2}+w_{2,2}}\\cdot e_{o2}$$ 使用矩阵乘法简化误差反向传播 误差向量：$$error_{output}=\\left(\\begin{matrix}e_1 \\\\e_2\\end{matrix}\\right)$$ 隐藏层误差：$$error_{hidden}=\\left(\\begin{matrix}\\frac{w_{1,1}}{w_{1,1}+w_{2,1}} &amp; \\frac{w_{1,2}}{w_{1,2}+w_{2,2}} \\\\\\\\\\frac{w_{2,1}}{w_{1,1}+w_{2,1}} &amp; \\frac{w_{2,2}}{w_{1,2}+w_{2,2}}\\end{matrix}\\right) \\cdot \\left(\\begin{matrix}e_1 \\\\ e_2 \\end{matrix} \\right)=\\left(\\begin{matrix}e_{h1}\\\\e_{h2}\\end{matrix}\\right)$$上述矩阵乘法太过复杂，无法通过简单的矩阵运算求解。观察上式可知，最重要的事情是输出误差链接权重$w_{j,k}$的乘法。较大的权重携带较大的误差给隐藏层，这些分数的分母是一种归一化因子。如果我们忽略掉这个因子，我们仅仅只是失去了后馈误差的真实值大小，但并没有失去其表示的真正含义（影响力）,也就是说反馈误差始终是以链接权重的强度来分配的。因此上式可以简化为：$$error_{hidden}=\\left(\\begin{matrix}w_{1,1} &amp; w_{1,2} \\\\w_{2,1} &amp; w_{2,2}\\end{matrix}\\right)\\cdot \\left(\\begin{matrix}e_1\\\\ e_2\\end{matrix}\\right)=\\left(\\begin{matrix}e_{h1}\\\\e_{h2}\\end{matrix}\\right)$$不难发现，隐藏层至输出层的链接权重矩阵$W_{hidden\\rightarrow output}$为:$$W_{hidden\\rightarrow output}=\\left(\\begin{matrix}w_{1,1} &amp; w_{2,1} \\\\w_{1,2} &amp; w_{2,2}\\end{matrix}\\right)=\\left(\\begin{matrix}w_{1,1} &amp; w_{1,2} \\\\w_{2,1} &amp; w_{2,2}\\end{matrix}\\right)^T$$因此：$$error_{hidden}=W_{hidden\\rightarrow output}^T\\cdot error_{output}=\\left(\\begin{matrix}w_{1,1} &amp; w_{2,1} \\\\w_{1,2} &amp; w_{2,2}\\end{matrix}\\right)^T\\cdot \\left(\\begin{matrix}e_1\\\\ e_2\\end{matrix}\\right)=\\left(\\begin{matrix}e_{h1}\\\\e_{h2}\\end{matrix}\\right)$$到此我们得到了用矩阵来传播误差的算法：$$error_{hidden}=W_{hidden\\rightarrow output}^T\\cdot error_{output}$$ 到此，我们已经做了大量的工作了。我们计算出了所有层的误差，接下来的工作就是根据误差来调整链接权重了。","link":"/2019/06/03/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E6%AF%94%E8%AE%B0%EF%BC%881%EF%BC%89/"},{"title":"【工具】HexoEditor编辑器的安装配置及使用","text":"HexoEditor 是一款多平台Markdown开源写作神器，在搭建完Hexo博客之后苦于发布博文的过程过于繁琐，而HexoEditor完美地解决了这一大痛点。其让写作与发布实现无缝衔接。GitHub：HexoEditor Step1: 安装 HexoEditor: 下载链接：Download 支持 Linux 、Windows 、MacOS Step2: 简单设置： Hexo配置文件选择网站根目录下的_config.yml文件 Tag模板目录选择网站很目录下的 /scaffolds 文件夹 主题随意选择 默认资源库可在网站根目录source文件夹内新建文件夹来存放文章图片资源 云图类型可默认，SM.MS云图为免费云端，也可自选其他云端服务 Step3:撰写文章： 点击新建Post可生成新的模板文档，开始写作。 在预览试图中右键点击所插入的图片，点击上传至SM.MS可将本地图片上传至云端，之后会发现文章中的图片Path变为URL链接。 Step4:发布文章： 右键 –&gt; Hexo –&gt; 部署网站","link":"/2018/12/27/HexoEditor/"},{"title":"人工神经网络学习笔记（0）","text":"何为人工神经网络人工神经网络是模拟人脑的神经网络，用以实现人工智能的机器学习技术。我们知道，人脑可以说是世界上最复杂最精妙的系统之一，它由千亿计的神经元细胞组成。各个神经细胞相互链接，彼此之间传递电信号。从而造就了人类高于其他物种的思维能力。科学家受到人脑神经元的启发从而提出了人工神经网络的设想，使得人工智能的实现不再遥不可及。 生物神经元关键部件： 树突 &amp; 胞体 &amp; 轴突 单个神经元的工作机制可以简单地描述为：树突接受其他神经元的神经末梢传来的电信号，信号传送到胞体并由某种机制决定是否激发下一次电信号的传递，若激发则电信号由轴突传递至神经末梢，再由神经末梢传递给其他神经元。其中，判断是否激发的机制有一大好处是可以减小神经元间微弱电信号（噪声）的干扰，使得自由足够强的电信号才能激发下一次传递。 人工神经元概览 由生物神经元得到的启发，人工神经元与其大同小异。上图中： $x_1,x_2,x_3,x_4$ 为该神经元树突所接受到的其他神经元传来的电信号。中间的圆圈为胞体，在胞体中将会由处理信号的机制，以决定输出信号y。功能 在之前的生物神经元中已经说道，神经元对是否激发信号传递有一个判断机制，这是因为神经元不希望传递微小的噪声信号，而只传递有意识的明显信号。只有信号强度达到了某一个阙值，才会激发电信号的传递。 那么在人工神经元中我们如何来实现这个机制呢？* 这样的一个函数也许能够满足我们的需要：$$Function(x) = \\begin{cases}0 &amp; x \\leq T \\\\1 &amp; x&gt;T\\end{cases}$$显而易见，这个简单的阶跃函数在输入信号大于T（阙值）时才会产生输出信号1（被激发），而较小的输入时输出为0（被抑制）；我们称这样的函数为激活函数。当然，激活函数不会就只有这么一种。常用的还有sigmoid函数：$$Sigmoid(x) = \\frac{1}{1+e^{-x}}$$函数图像：可以发现，sigmoid函数相对于阶跃函数而言更加平滑，自然，接近现实。我们的神经网络也将采用它。如此一来，我们的单个神经元模型就成型了：参数含义： $x_1 ,x_2… x_m$为输入信号 $w_1 ,w_2… w_m$为权重值，表示各个输入信号对输出结果的影响力大小。对于为何引入权值可以做如下思考：你去相亲，你对未来对象的考量主要有身高，长相，身材，文化程度等，但遇到样样都好的概率实在是太底了，所以你决定适当放宽某些要求。比如如果学历高就可以降低身材长相的要求。这表示你比较注重伴侣的文化程度。因此，对方的文化程度对你的择偶有着重要的影响，其所占权重就会比较高。 求和函数将计算$x=\\sum_{m}{x_iw_i},i=1,2,…m$后将所得的值传给激活函数Sigmoid即：$$Output=Sig(x)=\\frac{1}{1+e^{-(\\sum_{m}{x_iw_i})}}$$观察函数图不难发现，Sigmoid函数将加权求和的输入映射到0~1的值域内输出。人工神经网络介绍完单个神经元的功能，如果把这些小部件组合起来，就成了所谓的人工神经网络。 层次结构：* 输入层：接受外部输入信息，可以是图片等。 隐藏层：隐藏层层数不一，可根据需求来定。 输出层：将结果输出到外部。 链接方式： 除输入层外，每一层的每个神经元都接受其上一层所有神经元传来的信号的加权值。 神经元的链接方式并不唯一，你可以创造自己的链接方式，但为了便于抽象计算编码，规则的链接方式能帮我们大忙。 一个三层神经网络示例 参数释义： $i_1,i_2,i_3$为输入信号。 $w_{j,k}$表示后层结点$j$与前一层节点$k$之间链接的权重值。 $o_1,o_2,o_3$为该网络输出的结果信号。 信号的前向传播：前面我们介绍了单个人工神经元对信号的处理。但是现在网络中有多个神经元，我们当然不愿意对每一个神经元节点都进行编码计算，因此我们将其简化为矩阵运算。 将输入看成一个多维列向量：$$I=\\left[\\begin{matrix}i_1 \\\\i_2 \\\\i_3\\end{matrix} \\right]$$ 链接权重为一个$3\\times3$的矩阵：$$W_{input\\rightarrow hidden}=\\left[\\begin{matrix}w_{1,1} &amp; w_{2,1} &amp; w_{3,1} \\\\w_{1,2} &amp; w_{2,2} &amp; w_{3,2} \\\\w_{1,3} &amp; w_{2,3} &amp; w_{3,3}\\end{matrix} \\right]$$ 将两者相乘得到隐藏层的输入信号：$$I_{hidden}=W_{input\\rightarrow hidden}\\cdot I=\\left[\\begin{matrix}w_{1,1}\\cdot i_1 + w_{2,1}\\cdot i_2 + w_{3,1}\\cdot i_3 \\\\w_{1,2}\\cdot i_1 + w_{2,2}\\cdot i_2 + w_{3,2}\\cdot i_3 \\\\w_{1,3}\\cdot i_1 + w_{2,3}\\cdot i_2 + w_{3,3}\\cdot i_3\\end{matrix} \\right]$$也即：$$I_{hidden}=\\left[\\begin{matrix}\\sum_{j=1}^{3}{w_{j,1}\\cdot i_j} \\\\\\sum_{j=1}^{3}{w_{j,2}\\cdot i_j} \\\\\\sum_{j=1}^{3}{w_{j,3}\\cdot i_j}\\end{matrix}\\right]=[i_{h1},i_{h2},i_{h3}]^T$$ 再将加权求和的信号值经过Sigmoid激活函数处理我们可以得到最终的输出向量：$$O_{hidden}=Sig(W_{hidden\\rightarrow output}\\cdot I_{hidden})=\\left[\\begin{matrix}Sig(\\sum_{j=1}^{3}{w_{j,1}\\cdot o_{hj}}) \\\\Sig(\\sum_{j=1}^{3}{w_{j,2}\\cdot o_{hj}}) \\\\Sig(\\sum_{j=1}^{3}{w_{j,3}\\cdot o_{hj}}) \\\\\\end{matrix}\\right]=[o_{h1},o_{h2},o_{h3}]^T$$这里需要注意的是，输入层到隐藏层的链接权重矩阵与隐藏层到输出成的链接权重矩阵是不同的矩阵。但是计算过程是一致的，因此同理可得：$$I_{output}=Sig(W_{hidden\\rightarrow output}\\cdot O_{hidden})$$网络的最终输出为：$$O_{output}=\\left[\\begin{matrix}Sig(\\sum_{j=1}^{3}{w_{j,1}\\cdot i_{oj}}) \\\\Sig(\\sum_{j=1}^{3}{w_{j,2}\\cdot i_{oj}}) \\\\Sig(\\sum_{j=1}^{3}{w_{j,3}\\cdot i_{oj}}) \\\\\\end{matrix}\\right]=[o_1,o_2,o_3]^T$$到此，我们已经了解了神经网络中信号的前向传播机制，但是目前这个网络模型远远达不到我们的要求，它除了单纯的传播信号什么事情也做不了。显然后续我们得为其添加反馈机制，使其能够具有学习能力。","link":"/2019/06/03/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%880%EF%BC%89/"},{"title":"人工神经网络学习笔记（2）","text":"如何更新权重在上一篇文章中我们算出了各个层的误差，现在是时候利用这些误差来指导链接权重的修改了。那么该如何修改？ 暴力枚举：对于一个三层的神经网络，每层有3个神经元结点，有两个$3\\times 3$的链接权重矩阵，共有18个权重值。假设每个权重在1和-1之间共有1000种取值，那么我们有$1000^{18}$种权重组合，这个数字已经很大了。但是，如果是每层有500个结点呢？那么权重数将达到$2\\times 500\\times 500 = 500000$个，将会有$1000^{500000}$种组合。想要遍历这么些种可能得等到人类灭绝… 可见，暴力枚举并不能实际地解决我们的问题。 新的思路：让我们再次明确下我们的最终目的，让误差值降到最小。试着将其转化成数学上的求函数最小值问题。先前我们知道，误差是所有链接权重的函数:$$Error = F_{error}(w_{1,1},w_{2,1},……,w_{j,k})$$现在我们需要额就是找出该函数的最小值。但由于真正的误差函数的自变量太多，先举个简单的例子：假设误差函数只有一个自变量（链接权重）：$$E_{simple}=F_e(x)$$其图像为：可以将其想象成一个连绵的山脉，有山峰也有山谷。设想将一个小球至于山腰，那么在重力的作用下它必定沿着所在位置的斜率方向向下滚动直到山谷。但是很明显，我们并没有重力帮忙，因此必须人为指定“滚动方向”。不难发现，当斜率为正时应向左滚动（x–），斜率为负时应向右滚动（x++）。这种方法在数学上被称为梯度下降（gradient descent）。 可能的意外情况：*我们可能会碰到这种情况：当小球的起始位置为左侧山腰时，其很有可能最终会在局部最小值（左侧的山谷）停下，这可不是我们所希望的结果。因为我们的目的是把误差降到最小，那里显然不是最小的地方。为了避免上述情况，我们应从选择不同的其实位置对神经网络进行多次训练，以确保其并不总是终止于错误的地方。而不同的其实位置意味着不同的链接权重。 选择误差函数的形式可选项： $E=t_n-o_n$ (目标值 - 期望值) $E=(t_n-o_n)^2$ 方差形式 我们选用方差形式，因为其具有很多优点： 可以很容易地使用代数方法（链式法则求解偏导数）计算出梯度下降的斜率 误差函数平滑连续，这使得梯度下降算法可以很好地发挥作用 越接近最小值梯度（斜率）越小，按照斜率调整步长可以减少越过最佳位置的风险 计算梯度值（斜率） 当只有一个链接权重时，误差函数为二维曲线：$$k=\\frac{\\partial{E}}{\\partial{x}}$$ 当有两个链接权重时，误差函数为一个三维曲面：$$k=\\frac{\\partial{E}}{\\partial{w_{j,k}}}$$上述表达式表示了当权重$w_{j,k}$改变时，误差$E$是如何改变的。这是误差函数的斜率，也就是我们希望使用梯度下降的方法达到最小值的方向。 计算梯度： 在开始计算前我们回顾一下网络中各个参数的意义： 展开误差函数：由于一个结点的误差只与与其相连的链接权重有关，因此误差函数可以简单地表示为：$$E=(t_k-o_k)^2$$其中：$o_k=Sig(\\sum_{j=1}^{n}{w_{j,k}\\cdot o_{hj}})$所以：$$E=(t_k-Sig(\\sum_{j=1}^{n}{w_{j,k}\\cdot o_{hj}}))^2$$所以：$$\\frac{\\partial{E}}{\\partial{w_{j,k}}}=\\frac{\\partial{(t_k-o_k)^2}}{\\partial{w_{j,k}}}$$$$=\\frac{\\partial{E}}{\\partial{o_k}}\\cdot \\frac{\\partial{o_k}}{\\partial{w_{j,k}}}=-2(t_k-o_k)\\cdot \\frac{\\partial{o_k}}{\\partial{w_{j,k}}}$$其中：令$x=\\sum_{j=1}^{n}{w_{j,k}\\cdot o_{hj}}$$$\\frac{\\partial{o_k}}{\\partial{w_{j,k}}}=\\frac{\\partial{Sig(x)}}{\\partial{w_{j,k}}}=\\frac{\\partial{Sig(x)}}{\\partial{x}}\\cdot \\frac{\\partial{x}}{\\partial{w_{j,k}}}$$且：$$\\frac{\\partial{Sig(x)}}{\\partial{x}}=Sig(x)\\cdot (1-Sig(x))$$因此，我们得到了以下表达式：$$\\frac{\\partial{E}}{\\partial{w_{j,k}}}=-2(t_k-o_k)\\cdot Sig(\\sum_{j=1}^{n}{w_{j,k}\\cdot o_{hj}})\\cdot (1-Sig(\\sum_{j=1}^{n}{w_{j,k}\\cdot o_{hj}}))\\cdot \\frac{\\partial{(\\sum_{j=1}^{n}{w_{j,k}\\cdot o_{hj}}})}{\\partial{w_{j,k}}}$$$$=-2(t_k-o_k)\\cdot Sig(\\sum_{j=1}^{n}{w_{j,k}\\cdot o_{hj}})\\cdot (1-Sig(\\sum_{j=1}^{n}{w_{j,k}\\cdot o_{hj}}))\\cdot o_{hj}$$又由于我们只关心误差函数斜率的方向，因此可以将公式中的常数2省略，并不影响正负号：$$\\frac{\\partial{E}}{\\partial{w_{j,k}}}=-(t_k-o_k)\\cdot Sig(\\sum_{j=1}^{n}{w_{j,k}\\cdot o_{hj}})\\cdot (1-Sig(\\sum_{j=1}^{n}{w_{j,k}\\cdot o_{hj}}))\\cdot o_{hj}$$$$=-(e_j)\\cdot Sig(\\sum_{j=1}^{n}{w_{j,k}\\cdot o_{hj}})\\cdot (1-Sig(\\sum_{j=1}^{n}{w_{j,k}\\cdot o_{hj}}))\\cdot o_{hj}$$ 改变链接权重之前提到过，权重的改变方向与梯度的方向相反。因此我们规定权重的改变方式为：$$w_{j,k}^{new}=w_{j,k}^{old}-\\alpha \\cdot \\frac{\\partial{E}}{\\partial{w_{j,k}}}$$参数释义： $\\alpha$为学习因子，可以调节这些权重变化的强度 用矩阵来简化运算：$$\\left(\\begin{matrix}\\vartriangle w_{1,1} &amp; \\vartriangle w_{2,1} &amp; \\vartriangle w_{3,1} &amp; …\\\\\\vartriangle w_{1,2} &amp; \\vartriangle w_{2,2} &amp; \\vartriangle w_{3,2} &amp; …\\\\\\vartriangle w_{1,3} &amp; \\vartriangle w_{2,3} &amp; \\vartriangle w_{3,3} &amp; …\\\\… &amp; … &amp; … &amp; …\\end{matrix}\\right)=\\alpha\\cdot \\left(\\begin{matrix}e_1\\cdot S_1\\cdot (1-S_1)\\\\ e_2\\cdot S_2\\cdot (1-S_2)\\\\ e_k\\cdot S_k\\cdot (1-S_k)\\\\ …\\end{matrix}\\right) \\cdot (o_1,o_2,o_3,…)$$将Sig函数简化为输出：$$\\vartriangle w_{j,k} = \\alpha\\times E_k \\times O_k \\times (1-O_k) \\cdot O_j^T$$ 到此，所有的前期工作都已完成。","link":"/2019/06/04/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89/"},{"title":"博客迁移后的发布测试","text":"测试文章发布是否正常 迁移过程之后会整理出来，急需要的话可以留言。","link":"/2019/03/05/%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E5%90%8E%E7%9A%84%E5%8F%91%E5%B8%83%E6%B5%8B%E8%AF%95/"},{"title":"感知机与逻辑门的实现","text":"感知机 &emsp;感知机（Perceptron）由两层神经元组成，输入层接收外界信号后传递给输出层，输出层是M-P神经元（如下图），亦称” 阙值逻辑单元（threshold logic unit）“。 ![M-P神经元简图](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8E%E9%80%BB%E8%BE%91%E9%97%A8%E7%9A%84%E5%AE%9E%E7%8E%B0/20190624050228397.png) 逻辑门的实现 &emsp;感知机能容易地实现逻辑与、或、非、异或运算。注意到$y=f(\\sum_{i}{w_ix_i-\\theta})$,假定$f$为阶跃函数:$$sgn(x) = \\begin{cases}1, &amp; x&gt;= 0 \\\\0, &amp; x&lt;0\\end{cases}$$则有： 与门（$ x_1\\land x_2 $）：令$w_1=w_2=1,\\theta=1.5$,则$y=f(1\\cdot x_1+1\\cdot x_2-2)$,仅当$x_1=x_2=1$时，$y=1$ 123456789# python实现与门def AND(x1,x2): w1,w2,theta = 1.0,1.0,1.5 result = w1*x1 + w2*x2 - theta if result &gt;= 0: return 1 else: return 0 或门（$x_1\\lor x_2$）：令$w_1=w_2=1,\\theta=0.5$,则$y=f(1\\cdot x_1+1\\cdot x_2-0.5)$,仅当$x_1=1$或$x_2=1$时，$y=1$。12345678# python实现或门def OR(x1,x2): w1,w2,theta = 1.0,1.0,0.5 result = w1*x1 + w2*x2 - theta if result &gt;= 0: return 1 else: return 0 非门（$\\lnot x_1$）：令$w_1=-0.6$,$w_2=0,\\theta=-0.5$,则$y=f(-0.6\\cdot x_1+0\\cdot x_2+0.5)$,仅当$x_1=1$时，$y=0$；当$x_1=0$时，$y=1$。12345678# python 实现非门def NOT(x1): w1, theta = -0.6, -0.5 result = w1*x1 - theta if result &gt;= 0: return 1 else: return 0 与非门：令$w_1=w_2=-1,\\theta=-1.5$,则$y=f(1\\cdot x_1+1\\cdot x_2+1.5)$,仅当$x_1=x_2=1$时，$y=0$12345678# python 实现与非门def NAND(x1,x2): w1,w2,theta = -1.0,-1.0,-1.5 result = w1*x1 + w2*x2 - theta if result &gt;= 0: return 1 else: return 0 异或门（$x_1\\oplus x_2$）：异或门在生活中有很多用处，我们常常听到下面这句话：“这件事要么你来做，要么我来做”。意思是这件事只能由我们之间的任何一个人来做，其他都不行。又比如你房间中的灯的开关一般门口一个床头一个，这样子你每按下其中任何一个开关都只会有两种结果（变得与另一个一样，变得与另一个不一样）利用这两种结果就可以构成灯的开与关。 ![](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8E%E9%80%BB%E8%BE%91%E9%97%A8%E7%9A%84%E5%AE%9E%E7%8E%B0/20190624055322320.png) 123# python 实现异或门def ExOR(x1,x2): return AND(OR(x1,x2),NAND(x1,x2))","link":"/2019/06/01/%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8E%E9%80%BB%E8%BE%91%E9%97%A8%E7%9A%84%E5%AE%9E%E7%8E%B0/"},{"title":"【数据结构】树","text":"树的基本概念 树的定义：前面所提到的线性结构的元素是一种一对一的关系，而树是一种一对多的非线性结构，下面将通过一个具体的树的例子讲解到底什么样的结构才是树以 及树的一些相关术语： 上图就是个树结构的概图了，我们可以看到它是由唯一的一个根节点和若干棵互不相交的子树组成的，由此可知树的定义是地柜的，即在树的定义中又用到了树的定义。这里需要注意的是，树的结点数可以是零，若为零时，称为一棵空树。结点：上图中的每一个橙色圆圈都是结点，结点中不仅有数据域，还存在几个或零个指向其子树的指针；结点的度：该节点所引申出的分支数目，如上图中的B结点，向下引申出了E和F，故其度为2；树的度：树的度为树中所有结点的度的最大值，如上图树的度为A结点的度为3；叶子结点：又称为终端结点，指的是度为零的结点非终端结点：又称为分支结点，指的是度不为零的结点，上图中的ABCDEG都是非终端结点。另外，非终端结点除去根节点A之外的所有结点又称为内部结点。祖先：从树的根节点到某一节点的路径上的所有结点都成为该结点的祖先结点，如E的祖先节点为：A和B，因为路径为：A–B–E；层次：根处为第一层，以此类推……，如上图树的层次为4层；结点的深度和高度：联系实际，只需记住，高度是从底往上数；而深度是从上往下数；比如结点B的高度为3；而深度为2。根节点A的高度为树的高度为4；有序树：树中结点的子树从左到右都是有次序的不能交换无序树：树中结点的子树没有次序，可以任意交换丰满树：理想的平衡树，要求除了最底层外，其他层都是满的森林：若干棵互不相交的树的集合，若吧上图中的根节点A去掉，就成了一个森林 树的存储结构 顺序存储：双亲存储结构，亦称双亲表示法(克鲁斯卡尔算法) 链式存储：孩子存储结构（孩子表示法）孩子兄弟存储结构（孩子兄弟表示法） 二叉树 二叉树的五种基本形式 满二叉树的概念 完全二叉树的概念二叉树的几个重要性质 性质一：非空二叉树上叶子结点的数量等于双分支结点数加 1，即为$ n_0 = n_2 + 1$； 性质二：二叉树的第$i$层上最多有$2^{i-1}$; 性质三：高度（或深度）为$k$的二叉树最多有$2^k-1$个结点，换句话说就是一个深度为k的满二叉树的结点为$2^k-1$. 性质四：该性质与二叉树的顺序存储结构相关，在下面会提到，这里不再赘述； 性质五：函数$Catalan( ):$给定$n$个结点，能构成$h(n)$种不同的二叉树，其中：$h(n)=\\frac{1}{n+1}×C_{2n}^n$ ; 性质六：具有$n$个结点的完全二叉树的高度（或深度）为$[\\log_2{n}]+1$(向下取整)； 二叉树的存储结构 顺序存储结构：用一个数组来存储完全二叉树，将完全二叉树的结点值按编号依次存入一个一维数组中。 如果要从一维数组中还原二叉树的本来结构，按照以下规则：$i$ 为某结点的编号，若$i\\not=1$，则该结点的双亲结点的编号为$i/2$向下取整；如果$2i\\leq n$,则该结点的左孩子编号为$2i$,否则该结点没有左孩子；如果$2i+1 \\leq n$,则该结点的右孩子编号为$2i+1$;否则该结点无右孩子；下面给出将数组还原成二叉链树的代码： 123456789101112131415161718void creatBTree(int BT[],int n,BTNode *&amp;e){ BTNode *BTNode_array[maxsize]; for(int i = 1;i &lt;= n;++i ){ BTNode_array[i] = (BTNode*)malloc(sizeof(BTNode)); BTNode_array[i] -&gt; data = BT[i]; BTNode_array[i] -&gt; lchild = NULL; BTNode_array[i] -&gt; rchild = NULL; } for (int i = 1;i &lt;= n;++i){ if(2*i &lt;= n ){ BTNode_array[i] -&gt; lchild = BTNode_array[2*i]; } if((2*i+1) &lt;= n){ BTNode_array[i] -&gt; rchild = BTNode_array[2*i+1]; } } e = BTNode_array[1];} 链式存储结构：为了能够存储任意形式的二叉树结构，且根据二叉树一对多的非线性关系，设计出了二叉树的链式存储结构，结点定义如下： 12345typedef struct BTNode{ int data; //数据域 struct BTNode *lchild;//指向左孩子的指针 struct BTNode *rchild;//指向右孩子的指针}; 二叉树的遍历算法(递归实现)： 深度优先遍历：* 前序遍历： 1234567void preorder(BTNode *p){ if(p != NULL){ Visit(p); preorder(p -&gt; lchild); preorder(p -&gt; rchild); }} 中序遍历： 1234567void inorder(BTNode *p){ if(p != NULL){ inorder(p -&gt; lchild); Visit(p); inorder(p -&gt; rchild); }} 后序遍历： 1234567void postorder(BTNode *p){ if(p != NULL){ postorder(p -&gt; lchild); postorder(p -&gt; rchild); Visit(p); }} 广度优先遍历：* 算法流程图解： 1234567891011121314151617181920212223void level (BTNode *p){ if(p != NULL){ BTNode *que[maxsize]; int front = 0; int rear = 0; BTNode *q; rear = (rear + 1)%maxsize; que[rear] = p; while(front != rear){ front = (front + 1)%maxsize; q = que[front]; Visit(q); if(q -&gt; lchild != NULL){ rear = (rear + 1)%maxsize; que[rear] = q -&gt; lchild; } if(q -&gt; rchild != NULL){ rear = (rear + 1)%maxsize; que[rear] = q -&gt; rchild; } } }} 二叉树的遍历算法(非递归实现) 上图而查处将作为例子方便讲解下面的算法 深度优先遍历算法： 先序遍历： 123456789101112131415161718void preorderNonrecursion(BTNode *bt){ if(bt != NULL){ BTNode *Stack[maxsize]; int top = -1; BTNode *p; Stack[++top] = bt; while(top != NULL){ p = Stack[top--]; Visit(p); if(p -&gt; rchild != NULL){ Stack[++top] = p -&gt; rchild; } if(p -&gt; lchild != NULL){ Stack[++top] = p -&gt; lchild; } } }} 后序遍历：先序遍历算法遍历例子二叉树将得到：A B D E C F G后序遍历例子二叉树：D E B F G C A将后序遍历序列逆置：A C G F B E D可以发现，如果将先序遍历序列中对左右子树的遍历顺序交换一下，就可以得到逆后序遍历序列，再将逆后序序列逆置即可得到后序遍历序列。 123456789101112131415161718192021222324void postorderNonrecursion(BTNode *bt){ if(bt != NULL){ BTNode Stack1[maxsize]; BTNode Stack2[maxsize]; int top1 = -1; int top2 = -1; BTNode *p = NULL; Stack1[++top1] = bt; while(top1 != -1){ p = Stack1[top--]; Stack2[++top] = p; if(p -&gt; lchild != NULL){ Stack1[++top] = p -&gt; lchild; } if(p -&gt; rchild != NULL){ Stack1[++top] = p -&gt; rchild; } } while(top2 != -1){ p = Stack2[top--]; Visit(p); } }} 中序遍历： 12345678910111213141516171819void inorderNonrecursion(BTNode *bt){ if(bt != NULL){ BTNode *Stack[maxsize]; int top = -1; BTNode *p; p = bt; while(top != -1 || p != NULL){ while(p -&gt; lchild != NULL){ Stack[++top] = p; p = p -&gt; lchild; } if(top != -1){ p = Stack[top--]; Visit(p); p = p -&gt; rchild; } } }} 线索二叉树 对于二叉链表存储结构，$n$个结点的二叉树有$n+1$个空链域，能不能把这些空链域有效地利用起来，使二叉树的遍历更加高效呢？答案是肯定的，这就是线索二叉树的由来。 线索二叉树的优势：二叉树被线索化后近似于一个线性结构，分支结构的遍历操作就被转化成了近似线性结构的遍历操作，通过线索的辅助使得寻找当前结点的前驱或者后继的效率大大提高。 线索二叉树的构造：* 结点定义：*123456typedef struct TBTNode{ int data; // 数据域 struct TBTNode *lchild; // 左孩子（前驱结点）指针 struct TBTNode *rchild; // 右孩子（后继结点）指针 int ltag , rtag; //线索标记}; 根据二叉树的遍历方式的不同，线索二叉树可以分为先序线索二叉树、中序线索二叉树和后序线索二叉树。对一棵二叉树中的所有结点的空指针按照某种遍历方式加上线索的过程叫做二叉树的线索化，被线索化的二叉树就称为线索二叉树。 通过中序遍历对二叉树线索化代码：*123456789101112131415void InThread(TBTNode *p,TBTNode *&amp;pre){ if(p != NULL){ InThread(p -&gt; lchild,pre); // 递归地线索化左子树 if(p -&gt; lchild == NULL){ // 建立当前结点前驱线索 p -&gt; lchild = pre; p -&gt; ltag = 1; } if(pre != NULL &amp;&amp; pre -&gt; rchild == NULL){ // 建立前驱结点的后继线索 pre -&gt; rchild = p; pre -&gt; rtag = 1; } pre = p; // pre 跟上 p，之后p会指向下一个结点 InThread(p -&gt; rchild,pre); // 递归地线索化右子树 }} 通过中序遍历线索二叉树的主程序如下：*12345678void creatTBTNode(TBTNode *root){ TBTNode *pre = NULL; if(root != NULL){ InThread(root,pre); //递归建立线索二叉树 pre -&gt; rchild = NULL; // 处理最后一个结点 pre -&gt; rtag = 1; }} 遍历中序线索二叉树：* 寻找中序线索二叉树 root 的第一个遍历结点：*123456TBTNode *First(TBTNode *root){ while( root -&gt; ltag == 0){ root = root -&gt; lchild; } return root;} 在中序线索二叉树中，求结点p在中序下的后继结点的算法：*1234567TBTNode *Next(TBTNode *p){ if(p -&gt; rtag == 0){ return First(p -&gt; rchild); } else return p -&gt; rchild; // rtag = 1; 直接返回后继线索} 在中序线索二叉树上执行中序遍历：*12345void Inoreder(TBTNode *root){ for (TBTNode *p = First(root);p != NULL;p = Next(p)){ Visit(p); }} 通过前序遍历对二叉树线索化*12345678910111213141516171819void preInThread(TBTNode *p,TBTNode *&amp;pre){ if (p != NULL){ if(p -&gt; lchild != NULL){ p -&gt; lchild = pre; p -&gt; ltag = 1; } if (pre != NULL &amp;&amp; pre -&gt; rchild == NULL){ pre -&gt; rchild = p; pre -&gt; rtag = 1; } pre = p; if (p -&gt; ltag == 0){ preThread(p -&gt; lchild,pre); } if (p -&gt; rtag == 0){ preThread(p -&gt; rchild,pre); } }} 遍历前序线索二叉树算法：*12345678910111213void preorder(TBTNode *root){ if(root != NULL){ TBTNode *p = root; while(p != NULL){ while(p -&gt; ltag == 0){ Visit(p); p = p -&gt; lchild; } Visit(p); p = p -&gt; rchild; } }} 树与森林的互相转换 树与二叉树的应用 二叉排序树与二叉平衡树 赫夫曼树与赫夫曼编码 赫夫曼树的相关概念及介绍：赫夫曼树又被称为最优二叉树，它的特点是带权路径最短。下面介绍几个相关概念： 路径：指从树中一个结点到另一个结点的分支所构成的路线 路径长度：路径上的分支数目 树的路径长度：从根到每个结点的路径长度之和 带权路径长度：结点具有权值，从该结点到根结点的路径长度乘于该结点的权值就是该结点的带权路径长度 树的带权路径长度（WPL）：树中所有叶子结点的带权路径长度之和 赫夫曼树的构造方法：给定$n$个权值，用这些个权值来构造赫夫曼树：首先了解一下赫夫曼树的一些特点： 权值越大的结点距离根节点越近 树中没有度为1的结点 树的带权路径长度最短 根据上述特点反推我们是如何来构造一个赫夫曼树： 从最底层开始构造 最底层结点离根节点月远故其路径长度最大，而为使其带权路径长度最短，应选择权值最小的结点作为最底层结点 权值最大的结点离根节点最近 有了上述导论我们就可以开始构造一棵赫夫曼树了：上图构建的赫夫曼树的WPL为：$8×1+7×2+5×3+4×4+2×4=61$,这是这些结点所能构造的所有不同的树中树的带权路径长度最小的构造方式。 赫夫曼编码： * 利用赫夫曼树的特点来对文件进行压缩存储 看个例子：如果有这样一串字符将要被存储于计算机中$AECCBCDEEDECCCBAEEEBECDDBB$选三位长度的二进制数为A到E编码：根据上表我们可以将该字符串编码为：$000100010010001010011100100011100010010010001000100100100001000100100100001100010011011001001$解码时每三位一个字符解码总共需要78位来存储这个字符串，是否有根节省空间的编码形式呢？ 答案是肯定的，我们统计一下这个字符串中各个字符出现的频率（权值）：利用上表的信息构建一棵霍夫曼树，并将树中每个结点的左右分支进行编号（左0右1）：到此我们得到了对A到E的霍夫曼编码规则：根据上表的编码规则可将字符串编码为：$1110010101101011110011110101010110111000011001011111111110110$只需61位的空间就能存储该字符串，可以直观地发现比普通编码短了很多。 解码霍夫曼编码：解码霍夫曼编码需要用到上诉的那棵霍夫曼树，从左至右依次读取字符串编码，从根结点开始，读取到1则向右分支走，读取到0则向左分支走，直到走到叶子结点并读取该结点。 [注] 对于同一组结点，构造出的霍夫曼树是不唯一的。但是，得到的不同的霍夫曼树的WPL却是相同的 霍夫曼$n$叉树 霍夫曼树不都是二叉树，霍夫曼二叉树只是霍夫曼$n$叉树的一种特例 构造霍夫曼$n$叉树的逻辑与构造霍夫曼二叉树并无二异，我们知道对于结点数目大于等于2的待处理序列都可以构造霍夫曼二叉树。但却不一定能用来构造霍夫曼n叉树，构造n叉树的结点数目要求为大于等于3的奇数，若非奇数可以加上一个权值为0的结点。","link":"/2019/03/05/BinaryTree/"},{"title":"人工神经网络学习笔记（3）","text":"Python-numpy编码实现人工神经网络 前面的几篇文章我们熟悉了人工神经网络的数学原理及其推导过程，但有道是‘纸上得来终觉浅’，是时候将理论变为现实了。现在我们将应用Python语言以及其强大的扩充程序库Numpy来编写一个简单的神经网络。 准备数据： 训练集and测试集：Mnist手写数字数据集(复制git链接克隆)MINST数据库是由米国机器学习大佬Yann提供的手写数字数据库文件，其官方下载地址Download Mnist。该数据集将会是神经网络的输入信号。 每一张图片像素都为$28\\times 28$,因此可作为一个$784\\times 1$的向量传入神经网络。 初始花链接权重矩阵：使用正态概率分布采样权重，平均值为0，标准方差为结点传入链接数目的开方，即$\\frac{1}{\\sqrt{inputconnects}}$ 编码实现下面的代码实现了一个双隐层的神经网络，但是它的表现并不好（最起码在Mnist数据集的表现上差强人意），我训练了5个小时（5世代）也只能达到%96.54的准确率。相比而言当隐层的神经网络在Mnist数据集上的表现更好，三个小时（5世代）可以达到%97.34的准确率。你可以注释掉下面的部分代码将其退回到单隐层结构甚至加到三隐层结构。虽然代码写的很乱但代码中每一句都有详细的注释，别介意哈哈哈。包含两个源代码文件： neural_network.py 包含神经网络主类用于训练神经网络 network_test.py 用于测试神经网络 neural_network.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240import numpyimport matplotlib.pyplot as pltimport scipy.specialimport scipy.ndimage.interpolationimport timeimport progressbarimport matplotlib.animation as anim# 神经网络类定义class neuralNetwork: # 初始化神经网络 def __init__(self,inputnodes,hiddennodes,hiddennodes_2,outputnodes,learningrate): # 设置神经网络的输入层、隐藏层、输出层、的结点数和学习率 self.inodes = inputnodes self.hnodes = hiddennodes # 第一隐藏层结点数 self.hnodes_2 = hiddennodes_2 # 第二隐藏层结点数 #self.hnodes_3 = hiddennodes_3 # 第三隐藏层结点数 self.onodes = outputnodes # 学习率 self.lr = learningrate # （常规版）链接权重矩阵,随机权重在-0.5至0.5之间（三层神经网络） self.wih = (numpy.random.rand(hiddennodes,inputnodes)-0.5) self.who = (numpy.random.rand(outputnodes,hiddennodes)-0.5) # （进阶版）链接权重矩阵,随机权重在-0.5至0.5之间（三层神经网络） self.wih_ = numpy.random.normal(0.0,pow(self.hnodes,-0.5),(hiddennodes,inputnodes)) # 输入层到第一隐藏层权重矩阵 self.wh12_ = numpy.random.normal(0.0,pow(self.hnodes_2,0.5),(hiddennodes_2,hiddennodes)) # 第一隐藏层到第二隐藏层权重矩阵 #self.wh23_ = numpy.random.normal(0.0,pow(self.hnodes_3,0.5),(hiddennodes_3,hiddennodes_2)) # 第二隐藏层到第三隐藏层权重矩阵 self.who_ = numpy.random.normal(0.0,pow(self.onodes,-0.5),(outputnodes,hiddennodes_2)) # 第三隐藏层到输出层权重矩阵 #定义激活函数，由scipy库提供 self.activation_function = lambda x : scipy.special.expit(x) # 训练神经网络 def train(self,inputs_list,targets_list): # 将输入信号列表和目标信号列表转换成列向量 inputs = numpy.array(inputs_list,ndmin=2).T targets = numpy.array(targets_list,ndmin=2).T # 第一隐藏层的输入信号： hidden_inputs = numpy.dot(self.wih_,inputs) # 第一隐藏层的输出信号（激活函数作用）： hidden_outputs = self.activation_function(hidden_inputs) # 第二隐藏层的输入信号: hidden_inputs_2 = numpy.dot(self.wh12_,hidden_outputs) # 第二层隐藏层的输出信号： hidden_outputs_2 = self.activation_function(hidden_inputs_2) ''' # 第三隐藏层的输入信号： hidden_inputs_3 = numpy.dot(self.wh23_,hidden_outputs_2) # 第三隐藏层的输出信号： hidden_outputs_3 = self.activation_function(hidden_inputs_3) ''' # 输出层的输入信号： final_inputs = numpy.dot(self.who_,hidden_outputs_2) # 输出层的输出信号： final_outputs = self.activation_function(final_inputs) # 计算输出层误差向量 output_errors = targets - final_outputs # 计算第三隐藏层误差向量 #hidden_errors_3 = numpy.dot(self.who_.T,output_errors) # 计算第二隐藏层的误差向量 hidden_errors_2 = numpy.dot(self.who_.T,output_errors) # 计算第一隐藏层的误差向量 hidden_errors = numpy.dot(self.wh12_.T,hidden_errors_2) ''' 优化链接权重值 ''' # 第三隐藏层与输出层间的链接权重优化 #self.who_ += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)),numpy.transpose(hidden_outputs_3)) # 第二隐藏层与第三隐藏层间的链接权重优化 self.who_ += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)),numpy.transpose(hidden_outputs_2)) # 第一隐藏层与第二隐藏层间的链接权重优化 self.wh12_ += self.lr * numpy.dot((hidden_errors_2 * hidden_outputs_2 * (1.0 - hidden_outputs_2)),numpy.transpose(hidden_outputs)) # 输入层与第一隐藏层间的链接权重优化 self.wih_ += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)),numpy.transpose(inputs)) #return self.query(inputs_list) # 查询 def query(self,inputs_list): # 将输入列表转成numpy向量对象并转置为列向量 inputs = numpy.array(inputs_list,ndmin=2).T # 第一隐藏层结点的输入信号：权重矩阵与输入信号向量的乘积 self.hidden_inputs = numpy.dot(self.wih_,inputs) # 第一隐藏层结点的输出信号：经过S函数的加权求和值 self.hidden_outputs = self.activation_function(self.hidden_inputs) # 第二隐藏层的输入信号: self.hidden_inputs_2 = numpy.dot(self.wh12_,self.hidden_outputs) # 第二层隐藏层的输出信号： self.hidden_outputs_2 = self.activation_function(self.hidden_inputs_2) ''' # 第三隐藏层的输入信号： self.hidden_inputs_3 = numpy.dot(self.wh23_,self.hidden_outputs_2) # 第三隐藏层的输出信号： self.hidden_outputs_3 = self.activation_function(self.hidden_inputs_3) ''' # 输出层结点的输入信号： self.final_inputs = numpy.dot(self.who_,self.hidden_outputs_2) # 输出层结点的最终输出信号： self.final_outputs = self.activation_function(self.final_inputs) # 返回最终输出信号 return self.final_outputsdef test(Network,test_dataset_name): Network.wih_ = numpy.loadtxt('wih_file.csv') Network.wh12_ = numpy.loadtxt('wh12_file.csv') #Network.wh23_ = numpy.loadtxt('wh23_file.csv') Network.who_ = numpy.loadtxt('who_file.csv') # 准备测试数据 test_data_file = open(test_dataset_name,'r') test_data_list = test_data_file.readlines() test_data_file.close() print('\\n') print(\"Testing...\\n\") # 统计 correct_test = 0 all_test = 0 correct = [0,0,0,0,0,0,0,0,0,0] num_counter = [0,0,0,0,0,0,0,0,0,0] #测试进度条 p_test = progressbar.ProgressBar() p_test.start(len(test_data_list)) # 动画显示 #plt.figure(1) for imag_list in test_data_list: all_values = imag_list.split(',') lable = int(all_values[0]) scaled_input = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01 imag_array = numpy.asfarray(scaled_input).reshape((28,28)) ''' plt.imshow(imag_array,cmap='Greys',animated=True) plt.draw() plt.pause(0.00001) ''' net_answer = Network.query(scaled_input).tolist().index(max(Network.final_outputs)) num_counter[lable] += 1 if lable == int(net_answer): correct_test += 1 correct[lable] += 1 p_test.update(all_test + 1) all_test += 1 p_test.finish() print(\"Finish Test.\\n\") # 网络性能 performance = correct_test/all_test Per_num_performance = [] for i in range(10): # 测试集可能不包含某些数字，故捕捉除以0异常 try: Per_num_performance.append(correct[i]/num_counter[i]) except ZeroDivisionError: Per_num_performance.append(0) print(\"The correctRate of per number： \",Per_num_performance) print(\"Performance of the NeuralNetwork： \",performance*100) return performance# 定义网络规模与学习率input_nodes = 784hidden_nodes = 700hidden_nodes_2 = 700#hidden_nodes_3 = 100output_nodes = 10learningrate = 0.0001if __name__ == \"__main__\": # 定义训练世代数 epochs = 5 #创建神经网络实例 Net = neuralNetwork(input_nodes,hidden_nodes,hidden_nodes_2,output_nodes,learningrate) #plt.imshow(final_outputs,interpolation=\"nearest\") # 准备训练数据 data_file = open(\"mnist_train.csv\",'r') data_list = data_file.readlines() N_train = len(data_list) data_file.close() # 动画显示 #plt.figure(1) print(\"Training：\", epochs, \"epochs...\") for e in range(epochs): # 训练进度条 print('\\nThe '+str(e+1)+'th epoch trainning:\\n') p_train = progressbar.ProgressBar() p_train.start(N_train) i = 0 for img_list in data_list: # 以逗号分割记录 all_values = img_list.split(',') # 将0-255映射到0.01-0.99 scaled_input = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01 imag_array = numpy.asfarray(scaled_input).reshape((28,28)) #plt.imshow(imag_array,cmap='Greys',animated=True) #plt.draw() #plt.pause(0.00001) #旋转图像生成新的训练集 input_plus_10imag = scipy.ndimage.interpolation.rotate(imag_array,10,cval=0.01,reshape=False) input_minus_10imag = scipy.ndimage.interpolation.rotate(imag_array,-10,cval=0.01,reshape=False) input_plus10 = input_plus_10imag.reshape((1,784)) input_minus10 = input_minus_10imag.reshape((1, 784)) # 根据标签创建目标值向量 targets = numpy.zeros(output_nodes) + 0.01 targets[int(all_values[0])] = 0.99 # 用三个训练集训练神经网络 Net.train(scaled_input,targets) Net.train(input_plus10,targets) Net.train(input_minus10,targets) #time.sleep(0.01) p_train.update(i+1) i+=1 p_train.finish() print(\"\\nTrainning finish.\\n\") # 将训练好的神经网络链接权重输出到csv文件中 numpy.savetxt('wih_file.csv',Net.wih_,fmt='%f') numpy.savetxt('wh12_file.csv',Net.wh12_,fmt='%f') #numpy.savetxt('wh23_file.csv',Net.wh23_,fmt='%f') numpy.savetxt('who_file.csv',Net.who_,fmt='%f') network_test.py 12345678910111213141516171819202122import neural_network as nkimport numpyimport matplotlib.pyplot as plimport scipy.specialimport scipy.ndimage.interpolationimport jsonimport timeimport progressbar# 测试神经网络if __name__ == \"__main__\": input_nodes = nk.input_nodes hidden_nodes = nk.hidden_nodes hidden_nodes_2 = nk.hidden_nodes_2 #hidden_nodes_3 = nk.hidden_nodes_3 output_nodes = nk.output_nodes learningrate = nk.learningrate Network = nk.neuralNetwork(input_nodes,hidden_nodes,hidden_nodes_2,output_nodes,learningrate) nk.test(Network,\"mnist_test.csv\") # hidden_nodes = 200 lr = 0.01 performance = 97.34 运行程序 cd进入代码所在文件夹 (训练神经网络)输入命令：python neural_network.py （测试神经网络）输入命令：python network_test.py [Warning] 运行时请确保训练集和测试集数据的.csv文件与源代码文件在同一个目录下，否则请修改源码中的文件路径","link":"/2019/06/07/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%883%EF%BC%89/"},{"title":"【数据结构】数组、矩阵、和广义表","text":"数组： 常见的数组有一维数组和二维数组，二维数组是元素可以看成是一维数组的一维数组。对于数组主要考察元素下标计算的问题。对于一维数组较为简单，而对于二维数组的元素位置计算较为复杂，要考虑行优先和列优先两种情况。 二维数组的行优先和列优先存储： 行优先：从起始行开始一行一行地存入连续空间中 列优先：从起始列开始一列一列地存入连续空间中 矩阵的压缩存储： 矩阵的定义 矩阵一般用一个二维数组A[m][n]表示，表示一个m行n列的矩阵 其中m n 必须为常量，或者为预先定义的宏常量，如下：123#define m 5#define n 6int A[m][n]; 矩阵的一般操作与实现 矩阵的转置：1234567void transpose(int A[][maxsize],int B[][maxsize],int m,int n){ for(int i = 0; i &lt; m ; ++i){ for(int j = 0; j &lt; n ; ++j){ B[j][i] = A[i][j]; // 矩阵转置操作，元素关于主对角线互换位置 } }} 矩阵相加:1234567void addition(int A[][max],int B[][max],int C[][max],int m,int n){ for(int i = 0; i &lt; m ; ++i){ for(int j = 0; j &lt; n){ c[i][j] = A[i][j] + B[i][j]; //对应位置元素相加 } }} 矩阵相乘：12345678910void Multiply(int A[m][n],int B[n][k],int C[m][k],int m,int n,int k){ for(int i = 0;i&lt;m;++i){ for (int j = 0;j&lt;k;++j){ C[i][j] = 0; for(int h = 0;h&lt;n;++h){ C[i][j] += A[i][h] * B[h][j]; } } }} 特殊矩阵和稀疏矩阵 矩阵中绝大多数元素都是0的矩阵称为稀疏矩阵(国外教材) 相同的元素或者零元素在矩阵中的分布存在一定规律的矩阵称为特殊矩阵，反之称为稀疏矩阵(严版) 特殊矩阵：a) 对称矩阵 矩阵中的元素满足A[i][j] = A[j][i] 的矩阵称为对称矩阵 如上图所示，只需要存储一半的元素就可以了，要还原出另一半只需根据A[i][j] = A[j][i]这个条件就行了。 将一个n×n的对称矩阵存储在一维数组中，所需的存储空间为 $ \\dfrac{(1+n)·n}{2}$ 需要保存的元素为： 按照行优先来存储，保存在一维数组中，如下图所示：b)三角阵 上三角矩阵 为矩阵下三角部分(不包括对角线)元素全为零 下三角矩阵 为矩阵上三角部分(不包括对角线)元素全为零 三角矩阵的存储方式与对称矩阵类似，以下三角矩阵的存储为例，只需存储对角线及其以下部分的元素和其上三角中的一个元素C即可，如下图： c)对角矩阵 如下图所示的对角矩阵，其特点为除了主对角线以及其上下两条带状区域的元素外，其余元素都为C ( C可以为0)： 下面介绍如何求出第i行带状区域内的第一个元素在一维数组中的下标，假设c存在数组的最后一位： 当i=1时，带状区域内的第一个元素为矩阵当中的第一个元素，其在一维数组中的下标为0； 当i&gt;1时，第i行之前的元素个数为 $2+(i-2)×3$，则带状区域的第一个元素在一维数组中的下标为 $2+(i-2)×3$ 2.稀疏矩阵 稀疏矩阵中的相同元素c不像特殊矩阵中的相同元素的位置分布那么有规律可循，故必须为其设计一些特殊的存储结构 稀疏矩阵的顺序存储及其相关操作：常用的稀疏矩阵顺序存储方法有三元组表示法，和伪地址表示法。 三元组表示法：三元组数据结构为一个长度为n，表内每个元素都有三个分量的线性表，其三个分量分别为：“值”、“行下标”、“列下标”。元素结构体定义如下：12345typedef struct Trimat{ int val; // 值 int i; // 行下标 int j; // 列下标}； 结构体示意图：结构题数组的定义：1Trimat trimat[maxterms + 1]; // maxterms + 1;因为从第 1 行才开始存储元素 但是，为了方便起见，一般不用上诉结构体来定义三元组，直接申请一个二维数组就可以了： 123456789int trimat[maxterms + 1][3];// 如要求其他类型，可将int替换掉// 需要注意的是，如果矩阵是float型的（或者其他非整型的数据类型）// 则此时用一个数组来表示三元组应该写成如下形式：float trimat[maxterms + 1][3];// 这个时候若要取当前非零元素的所在位置应该这么做： (int)trimat[k][1]; (int)trimat[k][2];// 就是将float 型的元素造型成int型，这样就可以避免很多不必要的问题的发生 上诉定义方式中：trimat[k][0]表示原矩阵中的元素按行优先顺序的第k个元素的值trimat[k][1]、trimat[k][2]表示第k个非零元素在矩阵中的位置。事实上，trimat此时就是一个maxterms 行 3 列的二维数组，我们规定第0行的三个元素分别用来存储原矩阵中的非零元素个数，以及矩阵的行数与列数。示意图如下： 给定一个二维数组存储的矩阵，要求设计算法将其转化为三元组存储： 算法分析：建立一个三元组的核心问题在于求矩阵的非零元素个数以及非零元素的值，还有其在矩阵（原数组）中的位置，故只需扫描所给矩阵的二维数组即可得到相关数据，进而建立三元组。123456789101112131415161718// 建立三元组时，结点间的次序行按元素在矩阵中的行优先顺序排列void creattrimat(float A[][max],int m,int n,float B[][3]){// m,n 表示所给矩阵的规模为m×n int k = 1; for (int i = 0;i&lt;m;++i){ for (int j =0;j&lt;n;++j){ // 双重循环扫描矩阵 if (A[i][j] != 0){ // 若矩阵的[i][j] 上的元素不为零，将该元素连同其位置信息存入三元组中 B[k][0] = A[i][j]; B[k][1] = i; B[k][2] = j; k++; // k 指向三元组的下一个空间 } } } B[0][0] = k-1; // 将矩阵的基本信息存入三元组的第0行 B[0][1] = m; B[0][2] = n;} 设计算法打印出所给三元组存储的矩阵： 算法分析：读取三元组的第0行，得到矩阵的相关信息循环按行打印，若下标与三元组中的非零元素下标信息匹配则打印该非零元素，否则，打印0；1234567891011121314void print(float trimat[][3]){ int m = trimat[0][1]; int n = trimat[0][2]; int k = 1; // 非零元素从第一行开始存储 for(int i = 0;i&lt;m;i++){ // 双重循环打印矩阵 for(int j = 0;j&lt;n;j++){ // 循环过程中检查第[i][j]下标是否与三元组中的非零元素相同，若相同打印该非零元素，若不同打印 0 ； if(i = (int)trimat[k][1] &amp;&amp; j == (int)trimat[k][2]){ cout &lt;&lt; trimat[k][0] &lt;&lt; \" \"; ++k; } else cout &lt;&lt; \"0 \" &lt;&lt; endl; } }} 伪地址表示法：伪地址表示法与三元组表示法在本质上并无差别，只不过是三元组表示法的每一行用两个存储单元来存放原矩阵非零元素的位置标记，而伪地址表示法可以只用一个存储单元来存放位置标记，原因是因为对于一个$ m·n $的矩阵，伪地址表示法将元素位置下标的两个整数用一个公式映射( $ n·(i-1) +j $)到了一个整数上，同样利用该公式也可还原原i和j的值。我们来看一个例子： 稀疏矩阵的链式存储及相关操作： 邻接表表示法：邻接表表示法将矩阵中每一行的非零元素串联成一个单链表，链表结点中有三个分量：元素值、所在列、指针域结点的定义如下：12345typedef struct Listmat{ int data; int col; struct Listmat *next;}; 示意图如下：上图中最左端为一个指针数组，用来存储指向每一行非零元素单链表的头指针，数组下标为表示行标号。 十字链表表示法：在稀疏矩阵的十字链表存储结构中，矩阵的每一行用一个带头结点的链表表示，每一列也用一个带头结点的链表表示，这种存储结构中的链表结点有 5 个分量：行分量、列分量、数据域、指向下方结点的指针域、指向右方结点的指针域；结构图如下：普通结点定义：1234567typedef struct matNode{ int row; int col; int data; struct matNode *down; struct matNode *right;}; 头结点定义:1234typedef struct CrossList{ struct matNode *rhead,*cheard; // 指向两头结点数组的指针 int m,n,k; // 矩阵行数、列数、非零结点总数} 由于十字链表存储结构比较复杂，我们将通过其结构图例来深入了解它，图中附有详细的注释： 上图中，银灰色的结点为行头结点数组与列头结点数组，他们的定义如下： 1234567// 其中的m n 为矩阵的行数和列数typedef struct Rhead{ // 行头结点数组 struct matNode Rhead[m]; // 元素为matNode结点的数组};typedef struct Chead{ // 列头结点数组 struct matNode Chead[n];} 在理解了十字链表存储结构后，我们来看一个实际的应用：给定一个float型二维数组存储的稀疏矩阵，建立其对应的十字链表结构。算法分析：首先应该建立整体框架，也就是十字链表头结点以及行、列头结点数组。然后，按行优先顺序遍历矩阵数组，若发现不为零的元素，分配一个结点空间将其值存入，调整行、列头结点的指针域指向该结点，如此直到结束遍历123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354int creatCrossListmat(float A[][maxsize],int m,int n,int k,CrossList &amp;M){ //============================================ // 搭建基本框架操作 //============================================ // 清空处理： if (M.rhead) free(M.rhead); if (M.chead) free(M.chead); // 将矩阵信息存入十字链表头结点： M.m = m; M.n = n; M.k = k; // 申请行、列头结点数组空间： if (!(M.rhead = (matNode*)malloc(sizeof(matNode)*m))) return 0; if (!(M.chead = (matNode*)malloc(sizeof(matNode)*n))) return 0; // 将行、列头结点数组的right和down指针置空： for(int i = 0;i &lt; m ; ++i){ M.rhead[i].right = NULL; M.rhead[i].down = NULL; } for(int i = 0;i &lt; n ;++i){ M.chead[i].right = NULL; M.chead[i].down = NULL; } //=================================================== // 核心算法 //=================================================== // 建立链表的辅助指向列头结点的指针数组： matNode *temp_r[maxsize]; for(int i = 0;i&lt;n;++i){ temp_r[i] = &amp;(M.chead[i]); // 引用 } // 行优先顺序遍历矩阵数组构建十字链表： for(int i = 0;i &lt; m ;++i){ matNode *c = &amp;(M.rhead[i]); for(int j = 0;j &lt; n;++j){ if (A[i][j] != 0){ matNode *p = (matNode*)malloc(sizeof(matNode)); p -&gt; row = i; p -&gt; col = j; p -&gt; val = A[i][j]; p -&gt; down = NULL; p -&gt; right = NULL; // 如果某行（列）中已经连接了元素，那么就让这个元素充当该行（列）的头部，这样能使十字链表的行（列）头结点数组中的结点避免重复指向，覆盖；所以下面的代码所要做的事情很重要。 c -&gt; right = p; c = p; temp_r[j] -&gt; down = p; temp_r[j] = p; } } } return 1;} 广义表 表元素可以是原子类型的元素，也可以是广义表的一种线性表 三个重要概念： 广义表的长度：表中最上层元素的个数 广义表的深度：表中括号的最大层数，求解时应将所有的子表展开在分析 表头(Head)和表尾(Tail)：当表非空时，第一个元素为广义表的表头，其余元素组成广义表的表尾两种存储结构： 头尾链表存储结构：这种存储结构有两种结点，即原子结点和广义表结点。原子结点有两个域：标记域、数据域。广义表结点有三个域：标记域、头指针域、尾指针域其中头指针指向一个原子结点或广义表结点，尾指针为空或者指向 本层中 的下一个广义表结点，而标记域用来区分广义表结点(1)与原子结点(0);示意图入下： 扩展线性表结构：该种存储结构同样有两种结点，与头尾链表存储结构不同的是这里的原子结点有三个域：标记域、数据域、尾指针域。 示意图如下：*","link":"/2019/03/01/arrayandlist/"},{"title":"深度学习-卷积神经网络(0)","text":"&emsp;作为本系列的第一篇文章，本文仅对卷积神经网络的工作过程做一个简单的介绍，并不涉及数学原理与推导。若想要深入了解数学原理，那么可以去查看相关文献或者我将会在之后更新相关内容的文章。 &emsp;阅读这篇文章前你最好对简单的神经网络有一定的了解，如果没有，可以参看博主的神经人工神经网络学习笔记系列文章。如果你已经做好了准备，那就让我们开始吧！ 什么是卷积神经网络？ &emsp;卷积神经网络（Convolutional Neural Network，CNN）是前馈人工神经网络的一种。在图像识别领域有着广泛的应用并且非常有效。当人们谈到计算机视觉时，通常都绕不开卷积神经网络。 ![谷歌相册图像搜索](https://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/google.gif) 计算机眼中的图像 &emsp;毫无疑问，你可以很快分辨下图中的动物是只猫。但在计算机“眼中”，它仅仅是一个数字序列。图像由一个个像素组成，每一个像素通常以RGB(Red,Green,Blue)三原色表示。但为了简化，我们使用灰度（0-255）表示，仅仅一个数字就可以表示（0：黑色 255：白色）。如此一来，对于一张$200\\times 200$像素的图片，在计算机眼中就为一个$200\\times 200$的矩阵，也即一个$40000$维的向量。 ![计算机眼中的图像](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/20190619123229184.png) &emsp;计算机学习（训练）识别图像的过程就是将许多图片向量输入某种算法处理后将结果与目标值相比对，对误差进行修正直到结果输出令人满意为止。待训练结束后再给它看一个从未看过的图像它也能准确地识别图像的内容。 &emsp;本系列文章我们将继续使用Mnist数据集来训练和测试神经网络。 ![Mnist数据集](https://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/mnist_8.gif) LeNet框架（20世纪90年代） &emsp;LeNet框架是卷积神经网络的祖师爷LeCun在1998年提出的，用于解决手写数字识别的视觉任务。自那时起，CNN的最基本的架构就定下来了：卷积层、池化层、全连接层。本篇文章也将围绕该框架来进行卷积神经网络的介绍。 ![LeNet框架简图](https://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/lenet.jpg) LeNet 卷积神经网络的工作过程 ![卷积神经网络的计算过程](https://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/lenet_cal.png) 1、卷积运算： &emsp;顾名思义，卷积神经网络得名于“卷积”运算。在卷积神经网络中，卷积的主要目的是从目标图像中提取“特征”。通过使用输入数据中的小方块（矩阵分块）来学习图像特征，卷积运算保留了像素间的空间关系。&emsp;正如前文所说，每个图像都可以被计算机看成是一个像素值矩阵。现仅考虑一个$5\\times 5$像素的图像矩阵$W_{img}$： ![](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/20190619045802129.png) 再令一个$3\\times 3$的矩阵$W_{f}$： ![](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/20190619050403433.png) 将$3\\times 3$的矩阵在$5\\times 5$矩阵上移动并将对应位的数值相乘并求和，得到一个新的矩阵即为卷积运算后的特征值矩阵： ![卷积运算](https://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/cnn_image_sample.gif) &emsp;这个由特征值组成的矩阵被称为 卷积特征 或 特征映射 。而上述参与卷积运算的$3\\times 3$矩阵被称为 卷积滤波器 或 核 或 特征探测器 （以下统称滤波器，但是事实上过滤器的作用就是原始图像的 特征检测器）。上述例子中过滤器在图像矩阵上每次移动1个像素单位，称为 步幅 。 &emsp;不难发现，不同的滤波器作用于相同图像上会得到不同的特征映射，下图列出了一些滤波器的取值以及功能作用(边缘检测，锐化等)： ![](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/20190619053433213.png) &emsp;总的来说，一个滤波器在输入图像上移动（卷积操作）以生成特征映射。在同一张图像上，另一个滤波器的卷积生成了不同的特征图。需要注意到，卷积操作捕获原始图像中的局部依赖关系很重要。还要注意这两个不同的滤波器如何从同一张原始图像得到不同的特征图。请记住，以上图像和两个滤波器只是数值矩阵。&emsp;实际上，卷积神经网络在训练过程中会自己学习这些滤波器的值（尽管在训练过程之前我们仍需要指定诸如滤波器数目、大小，网络框架等参数）。我们拥有的滤波器数目越多，提取的图像特征就越多，我们的网络在识别新图像时效果就会越好。 特征映射（卷积特征）的大小由我们在执行卷积步骤之前需要决定的三个参数控制： 深度：深度对应于我们用于卷积运算的过滤器数量。在图6所示的网络中，我们使用三个不同的过滤器对初始的船图像进行卷积，从而生成三个不同的特征图。可以将这三个特征地图视为堆叠的二维矩阵，因此，特征映射的“深度”为3。 ![卷积操作](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/20190619054522781.png) 步幅：步幅是我们在输入矩阵上移动一次过滤器矩阵的像素数量。当步幅为1时，我们一次将过滤器移动1个像素。当步幅为2时，过滤器每次移动2个像素。步幅越大，生成的特征映射越小。 零填充：有时，将输入矩阵边界用零来填充会很方便，这样我们可以将过滤器应用于输入图像矩阵的边界元素。零填充一个很好的特性是它允许我们控制特征映射的大小。添加零填充也称为宽卷积，而不使用零填充是为窄卷积。 ![边界零填充](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/20190619055337351.png) 2、非线性操作（ReLU操作） &emsp;每次卷积操作之后，都会进行一次ReLU操作，其全称为修正线性单元（Rectified Linear Unit),是一种非线性操作。以下为修正线性函数的图像及表达式： $$f(x)=\\begin{cases}0, &amp; x&lt;0 \\\\x, &amp; otherwise\\end{cases}$$ ![修正线性函数](https://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/ReLU.png) &emsp;ReLU 是一个针对元素的操作（应用于每个像素），并将特征映射中的所有负像素值替换为零。ReLU 的目的是在卷积神经网络中引入非线性因素，因为在实际生活中我们想要用神经网络学习的数据大多数都是非线性的（卷积是一个线性运算 —— 按元素进行矩阵乘法和加法，所以我们希望通过引入 ReLU 这样的非线性函数来解决非线性问题）。从可以很清楚地理解 ReLU 操作。它展示了将 ReLU 作用于某个特征映射得到的结果。这里的输出特征映射也被称为“修正”特征映射。 ![修正线性函数](https://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/relu_op.png) 其他非线性函数如 Sigmoid 或 tanh 也能达到类似效果，但是 ReLU 函数的效果是最好的。 3、池化（Pooling） &emsp;空间池化（也称为子采样或下采样）可降低每个特征映射的维度，并保留最重要的信息。空间池化有几种不同的方式：最大值，平均值，求和等。 &emsp;在最大池化的情况下，我们定义一个空间邻域（例如一个2 × 2窗口），并取修正特征映射在该窗口内最大的元素。当然我们也可以取该窗口内所有元素的平均值（平均池化）或所有元素的总和。在实际运用中，最大池化 的表现更好。&ensp;下图展示了通过2 × 2窗口在修正特征映射（卷积+ ReLU 操作后得到）上应用最大池化操作的示例: ![Pooling](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/20190619072042438.png) &emsp;我们将2 x 2窗口移动2个单元格（也称为“步幅”），并取每个区域中的最大值。如图9所示，这样就降低了特征映射的维度,变成了一个$2\\times 2$的矩阵。&emsp;由于池化操作分别应用于每个特征映射（因此，我们从三个输入映射中得到了三个输出映射）。 ![在修正后的特征映射上应用池化](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/20190619072936964.png) 两种池化方法的结果对比： ![Pool Diffrent](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/20190619072355283.png) &emsp;池化的作用是逐步减少输入的空间大小。具体来说有以下四点： 使输入（特征维度）更小，更易于管理 减少网络中的参数和运算次数，因此可以控制过拟合 使网络对输入图像微小的变换、失真和平移更加稳健（输入图片小幅度的失真不会改池化的输出结果 —— 因为我们取了邻域的最大值/平均值） 可以得到尺度几乎不变的图像（确切的术语是“等变”）。这是非常有用的，这样无论图片中的物体位于何处，我们都可以检测到 &emsp;目前为止，我们已经了解了卷积神经网络中 卷积、ReLU、池化 的工作原理。这一点非常重要，下面我们将举例来描述这一过程。 可视化卷积神经网络 &emsp;Adam Harley 创建了一个基于 MNIST 手写数字数据集训练卷积神经网络的可视化。我强烈推荐大家 使用它来了解卷积神经网络的工作细节。其链接如下,可以自行尝试： 2D Visualization of a Convolutional Neural Network. ![](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/20190619074946589.png) 1、卷积层&emsp;将鼠标放在卷积层的某个像素点上并点击会出现： ![](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/20190619075722203.png) 2、池化层 ![](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/20190619080251028.png) 3、全连接层&emsp;全连接层的每一个结点都与其前一层的每一个结点相连接。 ![](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/20190619080952782.png) ![](http://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8%E7%BA%A7%E4%BB%8B%E7%BB%8D/20190619080746017.png) &emsp;该可视化项目还有3D版的，大家可以去玩玩，对于加深理解很有帮助。 卷积神经网络如何学习？ Waiting for update …","link":"/2019/06/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D(0)/"},{"title":"Hexo+icarus主题配置","text":"下载icarus主题 进入博客主目录，点击鼠标右键Git Bash Here,进入命令行界面 输入： 1git clone https://github.com/ppoffice/hexo-theme-icarus themes/icarus 打开themes文件夹，就会发现多了一个icarus文件夹，这就是主题的所有文件 配置主题 更改站点配置文件_config.yml,将主题改为icarus1theme: icarus Icarus文件目录概览： $config.yml$是主题的配置文件 $/layout$ 文件夹中是主题各种模板文件 我们主要的超作就是在这两个文件中了 主题配置文件（部分）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181# Version of the Icarus theme that is currently usedversion: 2.3.0# 你的网站图标，可以搜索在线图标制作，并将其放在images文件夹中favicon: /images/favicons.ico# Path or URL to RSS atom.xmlrss: /atom.xml# 显示在导航栏左侧的网站logo，同样可以自己制作logo: /images/gen.svg# Open Graph metadata# https://hexo.io/docs/helpers.html#open-graphopen_graph: # Facebook App ID fb_app_id: # Facebook Admin ID fb_admins: # Twitter ID twitter_id: # Twitter site twitter_site: # Google+ profile link google_plus: # 导航栏navbar: #菜单（显示名称：对应文件夹） menu: 主页: / 归档: /archives 分类: /categories 标签: /tags 关于: /about # 导航栏右侧图标链接 links: My GitHub: icon: fab fa-github url: '你的gityhub地址'# Footer section link settingsfooter: # 页脚图标链接 links: Creative Commons: icon: fab fa-creative-commons url: 'https://creativecommons.org/' Attribution 4.0 International: icon: fab fa-creative-commons-by url: 'https://creativecommons.org/licenses/by/4.0/' Download on GitHub: icon: fab fa-github url: 'http://github.com/ppoffice/hexo-theme-icarus'# 文章显示设置article: # Code highlight theme # https://github.com/highlightjs/highlight.js/tree/master/src/styles #代码主题atom-one-light亮色，atom-one-dark暗色 highlight: atom-one-dark # 是否显示文章主图 thumbnail: true # 是否显示估算阅读时间 readtime: true# 搜索插件设置# http://ppoffice.github.io/hexo-theme-icarus/categories/Configuration/Search-Pluginssearch: # Name of the search plugin type: insight# 评论插件设置# http://ppoffice.github.io/hexo-theme-icarus/categories/Configuration/Comment-Pluginscomment: #可选valine，disqus（科学上网）等 # Name of the comment plugin #type: valine #app_id: 不为空 #app_key: 不为空 #notify: true #verify: true #placeholder: type: disqus shortname: 不能为空# 打赏功能# http://ppoffice.github.io/hexo-theme-icarus/categories/Donation/donate: - # 阿里巴巴支付宝 type: alipay # 二维码图片 qrcode: '/images/honbao.PNG' - # 微信 type: wechat # 二维码图片 qrcode: '/images/yjtp.png' -# 分享插件设置# http://ppoffice.github.io/hexo-theme-icarus/categories/Configuration/Share-Pluginsshare: # 插件类型，有多种，可选，自行百度 type: sharejs# Sidebar settings.# Please be noted that a sidebar is only visible when it has at least one widgetsidebar: # 左侧边栏设置 left: # 是否不随页面滚动 # http://ppoffice.github.io/hexo-theme-icarus/Configuration/Theme/make-a-sidebar-sticky-when-page-scrolls/ sticky: false # 右侧边栏设置 right: # 是否不随页面滚动 # http://ppoffice.github.io/hexo-theme-icarus/Configuration/Theme/make-a-sidebar-sticky-when-page-scrolls/ sticky: false# 边栏小部件设置# http://ppoffice.github.io/hexo-theme-icarus/categories/Widgets/widgets: - # 个人信息 type: profile # 部件位置（左） position: left # 作者名（字符串） author: 飞鱼 # 作者身份描述（字符串） author_title: Student # 作者当前居住地 location: China,Fujian # 头像（可用本地图片或网络图片链接） avatar: '/images/ava.png' # Email address for the Gravatar to be shown in the profile widget gravatar: # 关注我的链接，可设为你的GitHub主页 follow_link: 'https://github.com/yourname' # 个人介绍部件底部图标社交链接 social_links: Github: icon: fab fa-github url: 'https://github.com/yourname' Facebook: icon: fab fa-facebook url: 'https://facebook.com' Twitter: icon: fab fa-twitter url: 'https://twitter.com/yourname' RSS: icon: fas fa-rss url: / - # Widget name type: toc # Where should the widget be placed, left or right position: left - # 分类 type: category # 位置指定 position: left - # 标签云 type: tagcloud # 位置 position: right - # 近期文章 type: recent_posts # 位置 position: left - # 归档 type: archive # Where should the widget be placed, left or right position: right - # 标签 type: tag # Where should the widget be placed, left or right position: right - # 外部链接 type: links # Where should the widget be placed, left or right position: left # Links to be shown in the links widget links: Google: 'https://google.com' Baidu: 'https://baidu.com' 上述设置已经让你的博客稍微有点属于你的样子了，下面来添加一些有意思的元素。 添加雪花飘落效果 在 $\\color{DarkTurquoise}{/themes/icarus/sourse/js/src}$目录下新建一个 snow.js 文件(若没有src/文件夹可以自己新建)，复制粘贴以下代码： 样式1：六边形雪花 **1234567891011121314151617181920212223242526272829303132333435363738394041424344(function($){ $.fn.snow = function(options){ var $flake = $('').css({'position': 'absolute','z-index':'9999', 'top': '-50px'}).html('❄'), documentHeight = $(document).height(), documentWidth = $(document).width(), defaults = { minSize : 10, maxSize : 20, newOn : 1000, flakeColor : \"#AFDAEF\" /* 此处可以定义雪花颜色，若要白色可以改为#FFFFFF */ }, options = $.extend({}, defaults, options); var interval= setInterval( function(){ var startPositionLeft = Math.random() * documentWidth - 100, startOpacity = 0.5 + Math.random(), sizeFlake = options.minSize + Math.random() * options.maxSize, endPositionTop = documentHeight - 200, endPositionLeft = startPositionLeft - 500 + Math.random() * 500, durationFall = documentHeight * 10 + Math.random() * 5000; $flake.clone().appendTo('body').css({ left: startPositionLeft, opacity: startOpacity, 'font-size': sizeFlake, color: options.flakeColor }).animate({ top: endPositionTop, left: endPositionLeft, opacity: 0.2 },durationFall,'linear',function(){ $(this).remove() }); }, options.newOn); };})(jQuery);$(function(){ $.fn.snow({ minSize: 5, /* 定义雪花最小尺寸 */ maxSize: 50,/* 定义雪花最大尺寸 */ newOn: 300 /* 定义密集程度，数字越小越密集 */ });});作者：donlex链接：http://www.imooc.com/article/272005 样式2：圆点状雪花 **123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129function snowFall(snow) { /* 可配置属性 */ snow = snow || {}; this.maxFlake = snow.maxFlake || 200; /* 最多片数 */ this.flakeSize = snow.flakeSize || 10; /* 雪花形状 */ this.fallSpeed = snow.fallSpeed || 1; /* 坠落速度 */}/* 兼容写法 */requestAnimationFrame = window.requestAnimationFrame || window.mozRequestAnimationFrame || window.webkitRequestAnimationFrame || window.msRequestAnimationFrame || window.oRequestAnimationFrame || function(callback) { setTimeout(callback, 1000 / 60); };cancelAnimationFrame = window.cancelAnimationFrame || window.mozCancelAnimationFrame || window.webkitCancelAnimationFrame || window.msCancelAnimationFrame || window.oCancelAnimationFrame;/* 开始下雪 */snowFall.prototype.start = function(){ /* 创建画布 */ snowCanvas.apply(this); /* 创建雪花形状 */ createFlakes.apply(this); /* 画雪 */ drawSnow.apply(this)}/* 创建画布 */function snowCanvas() { /* 添加Dom结点 */ var snowcanvas = document.createElement(\"canvas\"); snowcanvas.id = \"snowfall\"; snowcanvas.width = window.innerWidth; snowcanvas.height = document.body.clientHeight; snowcanvas.setAttribute(\"style\", \"position:absolute; top: 0; left: 0; z-index: 1; pointer-events: none;\"); document.getElementsByTagName(\"body\")[0].appendChild(snowcanvas); this.canvas = snowcanvas; this.ctx = snowcanvas.getContext(\"2d\"); /* 窗口大小改变的处理 */ window.onresize = function() { snowcanvas.width = window.innerWidth; /* snowcanvas.height = window.innerHeight */ }}/* 雪运动对象 */function flakeMove(canvasWidth, canvasHeight, flakeSize, fallSpeed) { this.x = Math.floor(Math.random() * canvasWidth); /* x坐标 */ this.y = Math.floor(Math.random() * canvasHeight); /* y坐标 */ this.size = Math.random() * flakeSize + 2; /* 形状 */ this.maxSize = flakeSize; /* 最大形状 */ this.speed = Math.random() * 1 + fallSpeed; /* 坠落速度 */ this.fallSpeed = fallSpeed; /* 坠落速度 */ this.velY = this.speed; /* Y方向速度 */ this.velX = 0; /* X方向速度 */ this.stepSize = Math.random() / 30; /* 步长 */ this.step = 0 /* 步数 */}flakeMove.prototype.update = function() { var x = this.x, y = this.y; /* 左右摆动(余弦) */ this.velX *= 0.98; if (this.velY &lt;= this.speed) { this.velY = this.speed } this.velX += Math.cos(this.step += .05) * this.stepSize; this.y += this.velY; this.x += this.velX; /* 飞出边界的处理 */ if (this.x &gt;= canvas.width || this.x &lt;= 0 || this.y &gt;= canvas.height || this.y &lt;= 0) { this.reset(canvas.width, canvas.height) }};/* 飞出边界-放置最顶端继续坠落 */flakeMove.prototype.reset = function(width, height) { this.x = Math.floor(Math.random() * width); this.y = 0; this.size = Math.random() * this.maxSize + 2; this.speed = Math.random() * 1 + this.fallSpeed; this.velY = this.speed; this.velX = 0;};// 渲染雪花-随机形状（此处可修改雪花颜色！！！）flakeMove.prototype.render = function(ctx) { var snowFlake = ctx.createRadialGradient(this.x, this.y, 0, this.x, this.y, this.size); snowFlake.addColorStop(0, \"rgba(255, 255, 255, 0.9)\"); /* 此处是雪花颜色，默认是白色 */ snowFlake.addColorStop(.5, \"rgba(255, 255, 255, 0.5)\"); /* 若要改为其他颜色，请自行查 */ snowFlake.addColorStop(1, \"rgba(255, 255, 255, 0)\"); /* 找16进制的RGB 颜色代码。 */ ctx.save(); ctx.fillStyle = snowFlake; ctx.beginPath(); ctx.arc(this.x, this.y, this.size, 0, Math.PI * 2); ctx.fill(); ctx.restore();};/* 创建雪花-定义形状 */function createFlakes() { var maxFlake = this.maxFlake, flakes = this.flakes = [], canvas = this.canvas; for (var i = 0; i &lt; maxFlake; i++) { flakes.push(new flakeMove(canvas.width, canvas.height, this.flakeSize, this.fallSpeed)) }}/* 画雪 */function drawSnow() { var maxFlake = this.maxFlake, flakes = this.flakes; ctx = this.ctx, canvas = this.canvas, that = this; /* 清空雪花 */ ctx.clearRect(0, 0, canvas.width, canvas.height); for (var e = 0; e &lt; maxFlake; e++) { flakes[e].update(); flakes[e].render(ctx); } /* 一帧一帧的画 */ this.loop = requestAnimationFrame(function() { drawSnow.apply(that); });}/* 调用及控制方法 */var snow = new snowFall({maxFlake:60});snow.start();作者：donlex链接：http://www.imooc.com/article/272005 最后在$\\color{DarkTurquoise}{/themes/icarus/layout/layout.ejs}$的$body$标签中添加代码：1234567&lt;!-- 雪花特效 --&gt; &lt;script type=\"text/javascript\"&gt; var windowWidth = $(window).width(); if (windowWidth &gt; 480) { document.write('&lt;script type=\"text/javascript\" src=\"/js/src/snow.js\"&gt;&lt;\\/script&gt;'); } &lt;/script&gt; 默认雪花为白色，可自行更改颜色，效果可见本站 网站访问量与访客量统计 不蒜子官网：http://busuanzi.ibruce.info/ 在$\\color{DarkTurquoise}{/themes/icarus/layout/common/footer.ejs}$模板文件的中添加如下代码：12345678&lt;span id=\"busuanzi_container_site_pv\" class=\"theme-info\"&gt; | 本站总访问量&lt;span id=\"busuanzi_value_site_pv\"&gt;span&gt;次 span&gt; &lt;span id=\"busuanzi_container_site_uv\" class=\"theme-info\"&gt; | 本站访客数&lt;span id=\"busuanzi_value_site_uv\"&gt;span&gt;人次 span&gt;&lt;script async src=\"//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js\"&gt;script&gt; 在$\\color{DarkTurquoise}{/themes/icarus/_config.yml}$中添加：12busuanzi: enable: true 最终效果： ![](https://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/Hexo+icarus主题配置/20190429093801600.png) ### 网站运行时间统计 在$\\color{DarkTurquoise}{/themes/icarus/layout/common/footer.ejs}$中添加：1234567891011121314151617&lt;span id=\"timeDate\"&gt;载入天数...&lt;/span&gt;&lt;span id=\"times\"&gt;载入时分秒...&lt;/span&gt; &lt;script&gt; var now = new Date(); function createtime() { var grt= new Date(\"12/28/2018 12:49:00\");//此处修改你的建站时间或者网站上线时间 now.setTime(now.getTime()+250); days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); if(String(hnum).length ==1 ){hnum = \"0\" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = \"0\" + mnum;} seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); snum = Math.round(seconds); if(String(snum).length ==1 ){snum = \"0\" + snum;} document.getElementById(\"timeDate\").innerHTML = \"本站已安全运行 \"+dnum+\" 天 \"; document.getElementById(\"times\").innerHTML = hnum + \" 小时 \" + mnum + \" 分 \" + snum + \" 秒\"; } setInterval(\"createtime()\",250); &lt;/script&gt; 修改自己的建站时间 最终效果： ![](https://hexoblog-1257022783.cos.ap-chengdu.myqcloud.com/Hexo+icarus主题配置/20190429094726470.png) 鼠标点击特效 在 $\\color{DarkTurquoise}{/themes/icarus/sourse/js/src}$中添加click.js文件，复制以下代码进去：123456789101112131415161718192021222324252627282930313233343536373839404142!function(e,t,a){ function n(){ c(\".heart{width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);}.heart:after,.heart:before{content: '';width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;}.heart:after{top: -5px;}.heart:before{left: -5px;}\"), o(), r() } function r(){ for(var e=0;e&lt;d.length;e++) d[e].alpha&lt;=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText=\"left:\"+d[e].x+\"px;top:\"+d[e].y+\"px;opacity:\"+d[e].alpha+\";transform:scale(\"+d[e].scale+\",\"+d[e].scale+\") rotate(45deg);background:\"+d[e].color+\";z-index:99999\"); requestAnimationFrame(r) } function o(){ var t=\"function\"==typeof e.onclick&amp;&amp;e.onclick; e.onclick=function(e){ t&amp;&amp;t(),i(e) } }function i(e){ var a=t.createElement(\"div\"); a.className=\"heart\",d.push({el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()}),t.body.appendChild(a) } function c(e){ var a=t.createElement(\"style\");a.type=\"text/css\"; try{ a.appendChild(t.createTextNode(e)) } catch(t){ a.styleSheet.cssText=e } t.getElementsByTagName(\"head\")[0].appendChild(a) } function s(){ return\"rgb(\"+~~(255*Math.random())+\",\"+~~(255*Math.random())+\",\"+~~(255*Math.random())+\")\" } var d=[]; e.requestAnimationFrame=function(){ return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e){ setTimeout(e,1e3/60) } }(), n()}(window,document); 最后在$\\color{DarkTurquoise}{/themes/icarus/layout/layout.ejs}$文件中的1&lt;!DOCTYPE html&gt; 的下一行添加：1&lt;script src=\"/js/src/click.js\"&gt;&lt;/script&gt; 效果见本站 看板娘插件 在博客主目录下进入点击进入Git Bash Here 输入命令：1npm install hexo-helper-live2d --save 在网站配置文件或主题配置文件$\\color{green}{_config.yml}$中添加：12345678910111213141516live2d: enable: true #开启看板娘 scriptFrom: local pluginRootPath: live2dw/ pluginJsPath: lib/ pluginModelPath: assets/ tagMode: false log: false model: use: live2d-widget-model-z16 #指定模型任务 display: position: right #显示位置 width: 200 #模型宽度 height: 400 #模型高度 mobile: show: true #是否在移动端显示 效果：见本站 遇到的坑及解决方法 ..公式渲染问题 .. .. $\\color{green}{Waiting....for....update....}$","link":"/2019/04/29/Hexo+icarus%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/"}],"tags":[{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Blog","slug":"Blog","link":"/tags/Blog/"},{"name":"ML","slug":"ML","link":"/tags/ML/"},{"name":"Tools","slug":"Tools","link":"/tags/Tools/"},{"name":"Tree","slug":"Tree","link":"/tags/Tree/"},{"name":"Array","slug":"Array","link":"/tags/Array/"}],"categories":[{"name":"教程","slug":"教程","link":"/categories/%E6%95%99%E7%A8%8B/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"工具","slug":"工具","link":"/categories/%E5%B7%A5%E5%85%B7/"},{"name":"数据结构","slug":"数据结构","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]}